{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:55:33.561114Z",
     "start_time": "2020-05-28T18:55:33.558117Z"
    }
   },
   "outputs": [],
   "source": [
    "#05.29 01:30ver.\n",
    "\n",
    "#validation_split 0.8\n",
    "#model = isensee_2017\n",
    "\n",
    "#write_data_to_file will be executed in the final step, with config[\"data_file\"]\n",
    "#fetch_data_to_file 대신 Naomi코드 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:55:37.448737Z",
     "start_time": "2020-05-28T18:55:33.563114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')\n",
      "Compute dtype: float16\n",
      "Variable dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "for i in tf.config.list_physical_devices():\n",
    "    print(i)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n",
    "\n",
    "#for multi-gpu usage\n",
    "#mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "\n",
    "#for mixed-precision training\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As crop_img and crop_img_to functions are missing in utils.py, we deactivated crop-related functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:55:37.503737Z",
     "start_time": "2020-05-28T18:55:37.449737Z"
    }
   },
   "outputs": [],
   "source": [
    "#unet3d/utils/sitk_utils.py\n",
    "#여기는 전처리 부분, 복셀간 원점, 좌표, 스페이싱, 방향, 보간법, transform, data <--> image 등을 하는 함수들을 정의\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "\n",
    "#복셀 원점 좌표 보정\n",
    "def calculate_origin_offset(new_spacing, old_spacing):\n",
    "    return np.subtract(new_spacing, old_spacing)/2\n",
    "\n",
    "\n",
    "#복셀간 공간 재조정(이미지, 스페이싱 (1, 1, 1), 선형보간법 사용, 기본값 = 0)\n",
    "#즉, 복셀간 공간을 재조정하여, resample_to_spacing 함수에 파라미터를 넘겨주고, 호출하는 함수\n",
    "def sitk_resample_to_spacing(image, new_spacing=(1.0, 1.0, 1.0), interpolator=sitk.sitkLinear, default_value=0.):\n",
    "    #줌팩터 = 기존 이미지로부터 얻은 공간간의 간격/새이미지로 부터 얻은 공간간의 간격\n",
    "    zoom_factor = np.divide(image.GetSpacing(), new_spacing)\n",
    "    #새로운 사이즈 = np.asarray(올림(반올림(줌팩터 * 이미지로부터 얻은 사이즈)), 소수점 다섯째짜리까지)\n",
    "    new_size = np.asarray(np.ceil(np.round(np.multiply(zoom_factor, image.GetSize()), decimals=5)), dtype=np.int16)\n",
    "    #오프셋 = (새로 만들 이미지의 복셀 간격, 이미지로부터 얻은 복셀 간격)\n",
    "    offset = calculate_origin_offset(new_spacing, image.GetSpacing())\n",
    "    #참조 이미지 = 빈이미지(새 사이즈, 새 복셀간 공간, 이미지 방향, 원점+(보정값), 디폴트값)\n",
    "    reference_image = sitk_new_blank_image(size=new_size, spacing=new_spacing, direction=image.GetDirection(),\n",
    "                                           origin=image.GetOrigin() + offset, default_value=default_value)\n",
    "    #이미지 재조정(이미지, 참조이미지, 보간법, 디폴트값)\n",
    "    return sitk_resample_to_image(image, reference_image, interpolator=interpolator, default_value=default_value)\n",
    "\n",
    "\n",
    "#이미지 재조정(이미지, 참조된 이미지, 디폴트값, 선형보간법, transform=None, output_pixel_type=None)\n",
    "def sitk_resample_to_image(image, reference_image, default_value=0., interpolator=sitk.sitkLinear, transform=None,\n",
    "                           output_pixel_type=None):\n",
    "    #transform에 아무값도 안넣어주면, sitk에 있는 Transform 사용 후, 항등행렬로 변환\n",
    "    if transform is None:\n",
    "        transform = sitk.Transform()\n",
    "        transform.SetIdentity()\n",
    "    #output_pixel 타입에 아무값도 안 넣어주면, 이미지의 GetPixelID()값(픽셀타입을 얻어오는함수)을 output_pixel_type에 대입하여 사용\n",
    "    if output_pixel_type is None:\n",
    "        output_pixel_type = image.GetPixelID()\n",
    "    resample_filter = sitk.ResampleImageFilter() #resample_filter에 sitkResampleImageFilter 적용\n",
    "    resample_filter.SetInterpolator(interpolator) #선형보간법 설정\n",
    "    resample_filter.SetTransform(transform) #transform 설정\n",
    "    resample_filter.SetOutputPixelType(output_pixel_type) #픽셀타입설정\n",
    "    resample_filter.SetDefaultPixelValue(default_value) #픽셀값 디폴트값으로 설정\n",
    "    resample_filter.SetReferenceImage(reference_image) #참조된 이미지 값에 적용\n",
    "    return resample_filter.Execute(image) #이미지 재조정 필터 실행\n",
    "\n",
    "\n",
    "#빈 복셀이미지 형성(사이즈, 복셀간의 공간, 방향, 원점, 디폴트값=0)\n",
    "def sitk_new_blank_image(size, spacing, direction, origin, default_value=0.):\n",
    "    #이미지 = 1로 구성된, 이미지배열을 받아, 전치행렬을 구한후, default값을 곱해서, 이미지 Array를 받아 image에 저장\n",
    "    image = sitk.GetImageFromArray(np.ones(size, dtype=np.float).T * default_value)\n",
    "    #복셀 이미지의 스페이싱, 다이렉션, 원점 정의\n",
    "    image.SetSpacing(spacing)\n",
    "    image.SetDirection(direction)\n",
    "    image.SetOrigin(origin)\n",
    "    return image\n",
    "\n",
    "#공간 재조정(데이터, 공간, 타겟으로 하는 공간, 선형보간법, 디폴트값)\n",
    "def resample_to_spacing(data, spacing, target_spacing, interpolation=\"linear\", default_value=0.):\n",
    "    #다음 함수를 실행해, image에 저장, 데이터를 이미지로 변환시켜, image에 저장\n",
    "    image = data_to_sitk_image(data, spacing=spacing)\n",
    "    #만약, 선형보간법을 사용할 경우\n",
    "    if interpolation is \"linear\":\n",
    "        interpolator = sitk.sitkLinear\n",
    "    #만약, 최단입점보간법을 사용할 경우(즉, output을 만들때, input에서 가장 가까운 놈(비슷한 놈)을 가져다 쓴다.)\n",
    "    elif interpolation is \"nearest\":\n",
    "        interpolator = sitk.sitkNearestNeighbor\n",
    "    #그외 값들은 에러처리\n",
    "    else:\n",
    "        raise ValueError(\"'interpolation' must be either 'linear' or 'nearest'. '{}' is not recognized\".format(\n",
    "            interpolation))\n",
    "    resampled_image = sitk_resample_to_spacing(image, new_spacing=target_spacing, interpolator=interpolator,\n",
    "                                               default_value=default_value)\n",
    "    return sitk_image_to_data(resampled_image)\n",
    "\n",
    "\n",
    "#data -> sitk_image\n",
    "def data_to_sitk_image(data, spacing=(1., 1., 1.)):\n",
    "    #3차원이면, 다음과 같이 회전\n",
    "    if len(data.shape) == 3:\n",
    "        data = np.rot90(data, 1, axes=(0, 2))\n",
    "    #data array를 받아, image에 저장\n",
    "    image = sitk.GetImageFromArray(data)\n",
    "    image.SetSpacing(np.asarray(spacing, dtype=np.float))\n",
    "    return image\n",
    "\n",
    "#sitk_image -> data\n",
    "def sitk_image_to_data(image):\n",
    "    #data에 이미지Array값을 저장\n",
    "    data = sitk.GetArrayFromImage(image)\n",
    "    #3차원이면, 다음과 같이 회전\n",
    "    if len(data.shape) == 3:\n",
    "        data = np.rot90(data, -1, axes=(0, 2))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:55:37.971242Z",
     "start_time": "2020-05-28T18:55:37.504737Z"
    }
   },
   "outputs": [],
   "source": [
    "#unet3d/utils/utils.py\n",
    "#이미지 파일들을 resize해서, 이미지들이나, 이미지 파일을 읽어오는 함수 정의\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import collections\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from nilearn.image import reorder_img, new_img_like\n",
    "\n",
    "#얘는 모르겠다.... 별로 건드리고 싶지도 않고, 건드릴 필요도 없을듯, shape형태를 교정해주는것인듯\n",
    "def fix_shape(image):\n",
    "    if image.shape[-1] == 1:\n",
    "        return image.__class__(dataobj=np.squeeze(image.get_data()), affine=image.affine)\n",
    "    return image\n",
    "\n",
    "#resize(이미지, 원하는 shape, 선형보간법)\n",
    "def resize(image, new_shape, interpolation=\"linear\"):\n",
    "    #아핀 대각선이 있는 이미지를 반환, 함수에서 반환되는 새로운 이미지의 방향은 RAS(Right, Anterior, Superior로 정의)\n",
    "    #너무 수학적인 내용이 들어갔다....\n",
    "    image = reorder_img(image, resample=interpolation)\n",
    "    #zoom 배율\n",
    "    zoom_level = np.divide(new_shape, image.shape)\n",
    "    #image.header.get_zooms()는 image의 복셀 크기를 mm단위로 얻는 함수, (2.0, 2.0, 2.19999..., 2000.0) 이런식으로 나온다는데,\n",
    "    #마지막 값은, 카메라 스캔간격을 표시한다고 한다. 스캔간격은 (miliseconds)\n",
    "    new_spacing = np.divide(image.header.get_zooms(), zoom_level)\n",
    "    #resample_to_spacing의 정의는 위의것을 참조, (image의 데이터, image의 복셀크기, newspacing, 보간법)\n",
    "    new_data = resample_to_spacing(image.get_data(), image.header.get_zooms(), new_spacing,\n",
    "                                   interpolation=interpolation)\n",
    "    #새 affine공간에 image공간 복사(affine공간이라고\n",
    "    #기하학에서, 아핀 공간은 유클리드 공간의 아핀 기하학적 성질들을 일반화해서 만들어지는 구조이다.\n",
    "    #핀 공간에서는 점에서 점을 빼서 벡터를 얻거나 점에 벡터를 더해 다른 점을 얻을 수는 있지만 원점이 없으므로 \n",
    "    #점과 점을 더할 수는 없다. 이렇게 나와있는데, 알듯말듯 한데, 체감이 잘 안옴\n",
    "    new_affine = np.copy(image.affine)\n",
    "    \n",
    "    #tolist는 배열을 목록으로 바꿔주는 함수\n",
    "    np.fill_diagonal(new_affine, new_spacing.tolist() + [1])\n",
    "    \n",
    "    #new_affine공간의 좌표를 보정\n",
    "    new_affine[:3, 3] += calculate_origin_offset(new_spacing, image.header.get_zooms())\n",
    "    \n",
    "    #참조 이미지와 동일한 클래스의 새 이미지를 만들어 반환, (참조이미지, 만들 이미지에 저장될 데이터, affin=new_affine행렬 적용)\n",
    "    return new_img_like(image, new_data, affine=new_affine)\n",
    "\n",
    "#이미지를 읽는함수, (파일, image_shape=None, 선형보간법, 추출)\n",
    "def read_image(in_file, image_shape=None, interpolation='linear', crop=None):\n",
    "    print(\"Reading: {0}\".format(in_file))\n",
    "    #파일 로드\n",
    "    image = nib.load(in_file) # J.Lee: nib.load(os.path.abspath(in_file)) -> nib.load(in_file)\n",
    "    image = fix_shape(image)\n",
    "#     if crop:\n",
    "#         image = crop_img_to(image, crop, copy=True)\n",
    "    #image_shape가 true, 즉, shape값이 있으면, resize함수 적용후 반환, 아니면, 그냥 반환\n",
    "    if image_shape:\n",
    "        return resize(image, new_shape=image_shape, interpolation=interpolation)\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "#이미지 파일들을 읽는 함수\n",
    "\n",
    "#함수에 들어있는 파라미터 의미는 다음과 같다.\n",
    "#image_files : 이미지파일들 경로\n",
    "#image_shape : 이미지 형태, 가로*세로*높이 = (240, 240, 155)\n",
    "#crop : 추출여부\n",
    "#label_indices : 예상되는 라벨이미지의 인덱스(1, 2, 4)\n",
    "#선형보간법을 사용하면, 레이블이 엉망이 되니, 건들지말라고 써있다!\n",
    "\n",
    "def read_image_files(image_files, image_shape=None, crop=None, label_indices=None):\n",
    "    \"\"\"\n",
    "    :param image_files: # J.Lee: paths of image_files.\n",
    "    :param image_shape: # J.Lee: (d,l,m)(155,240,240)\n",
    "    # J.Lee:\n",
    "    :param label_indices:\n",
    "        len(image_files)-1. i.e. expected index of label(truth) image \n",
    "        (when called through: write_image_data_to_file-> reslice_image-> read_image_files)\n",
    "    :param crop: \n",
    "    :param use_nearest_for_last_file: If True, will use nearest neighbor interpolation for the last file. This is used\n",
    "    because the last file may be the labels file. Using linear interpolation here would mess up the labels.\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if label_indices is None:\n",
    "        label_indices = []\n",
    "    elif not isinstance(label_indices, collections.Iterable) or isinstance(label_indices, str):\n",
    "        label_indices = [label_indices]\n",
    "    image_list = list()\n",
    "    for index, image_file in enumerate(image_files):\n",
    "        if (label_indices is None and (index + 1) == len(image_files)) \\\n",
    "                or (label_indices is not None and index in label_indices):\n",
    "            interpolation = \"nearest\"\n",
    "        else:\n",
    "            interpolation = \"linear\"\n",
    "        image_list.append(read_image(image_file, image_shape=image_shape, crop=crop, interpolation=interpolation))\n",
    "\n",
    "    return image_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:55:37.986243Z",
     "start_time": "2020-05-28T18:55:37.972242Z"
    }
   },
   "outputs": [],
   "source": [
    "#unet3d/normalize.py\n",
    "#슬라이스와 정규화 함수 정의\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from nilearn.image import new_img_like\n",
    "\n",
    "# from unet3d.utils.utils import resize, read_image_files\n",
    "# from .utils import crop_img, crop_img_to, read_image #!!!Missing in .utils!!!\n",
    "\n",
    "# def get_cropping_parameters(in_files):\n",
    "#     if len(in_files) > 1:\n",
    "#         foreground = get_complete_foreground(in_files)\n",
    "#     else:\n",
    "#         foreground = get_foreground_from_set_of_files(in_files[0], return_image=True)\n",
    "#     return crop_img(foreground, return_slices=True, copy=True)\n",
    "\n",
    "#슬라이스\n",
    "def reslice_image_set(in_files, image_shape, out_files=None, label_indices=None, crop=False):\n",
    "    if crop:\n",
    "        pass\n",
    "#         crop_slices = get_cropping_parameters([in_files])\n",
    "    else:\n",
    "        crop_slices = None\n",
    "    #이미지 파일들을 불러와서   \n",
    "    images = read_image_files(in_files, image_shape=image_shape, crop=crop_slices, label_indices=label_indices)\n",
    "    #Out_files가 None이 아닐경우, 즉, 존재할 경우, 다음을 실행\n",
    "    if out_files:\n",
    "        for image, out_file in zip(images, out_files):\n",
    "            #모든 이미지마다, filename을 붙여줌, 즉, out_file과 매칭\n",
    "            image.to_filename(out_file)\n",
    "        return [out_file for out_file in out_files]\n",
    "        # J.Lee: original: [os.path.abspath(out_file) for out_file in out_files]\n",
    "    else:\n",
    "        return images\n",
    "\n",
    "#data 정규화\n",
    "def normalize_data(data, mean, std):\n",
    "    #newaxis는 간단히 말해, 차원을 늘려주는것이라고 생각하면 된다.\n",
    "    #data = data - mean[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "    data -= mean[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "    #data = data / std[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "    data /= std[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "    return data\n",
    "\n",
    "#정규화된 데이터를 저장\n",
    "def normalize_data_storage(data_storage):\n",
    "    #빈 리스트 생성\n",
    "    means = list()\n",
    "    stds = list()\n",
    "    \n",
    "    for index in range(data_storage.shape[0]):\n",
    "        data = data_storage[index]\n",
    "        means.append(data.mean(axis=(1, 2, 3)))\n",
    "        stds.append(data.std(axis=(1, 2, 3)))\n",
    "    mean = np.asarray(means).mean(axis=0)\n",
    "    std = np.asarray(stds).mean(axis=0)\n",
    "    for index in range(data_storage.shape[0]):\n",
    "        data_storage[index] = normalize_data(data_storage[index], mean, std)\n",
    "    return data_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:55:38.121264Z",
     "start_time": "2020-05-28T18:55:37.987279Z"
    }
   },
   "outputs": [],
   "source": [
    "#unet3d/data.py\n",
    "#hdf5파일을 생성 함수와, 생성된 hdf5파일에 데이터셋 기록 및 추가, 읽어오는 함수가 정의된 부분인듯\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tables\n",
    "\n",
    "#from .normalize import normalize_data_storage, reslice_image_set\n",
    "\n",
    "#데이터 파일 생성 함수 정의\n",
    "def create_data_file(out_file, n_channels, n_samples, image_shape):\n",
    "    #hdf5파일 쓰기모드\n",
    "    hdf5_file = tables.open_file(out_file, mode='w')\n",
    "    #complevel : 필터속성의 컨테이너(데이터의 압축 수준을 지정, 허용되는 범위는 0-9, 값 0 (기본값)은 압축을 비활성화)\n",
    "    #complib : 사용할 압축 라이브러리\n",
    "    filters = tables.Filters(complevel=5, complib='blosc')\n",
    "    data_shape = tuple([0, n_channels] + list(image_shape)) # J.Lee: (155,240,240) -> [0,4,155,240,240]\n",
    "    truth_shape = tuple([0, 1] + list(image_shape)) # J.Lee: (155,240,240) -> [0,1,155,240,240]\n",
    "    \n",
    "    #딥러닝 훈련용, 하둡파일 생성 (본 코드에서는 hdf5로 생성)\n",
    "    data_storage = hdf5_file.create_earray(hdf5_file.root, 'data', tables.Float32Atom(), shape=data_shape,\n",
    "                                           filters=filters, expectedrows=n_samples)\n",
    "    truth_storage = hdf5_file.create_earray(hdf5_file.root, 'truth', tables.UInt8Atom(), shape=truth_shape,\n",
    "                                            filters=filters, expectedrows=n_samples)\n",
    "    affine_storage = hdf5_file.create_earray(hdf5_file.root, 'affine', tables.Float32Atom(), shape=(0, 4, 4),\n",
    "                                             filters=filters, expectedrows=n_samples)\n",
    "    return hdf5_file, data_storage, truth_storage, affine_storage\n",
    "\n",
    "#파일에 이미지 데이터 기록\n",
    "def write_image_data_to_file(image_files, data_storage, truth_storage, image_shape, n_channels, affine_storage,\n",
    "                             truth_dtype=np.uint8, crop=True):\n",
    "    for set_of_files in image_files:\n",
    "        # J.Lee: {set of files} includes paths of 1~4 channel images + 1truth image\n",
    "        #슬라이스된 이미셋을 images에 저장\n",
    "        images = reslice_image_set(set_of_files, image_shape, label_indices=len(set_of_files) - 1, crop=crop)\n",
    "        #images에 저장된 이미지에서 data를 뽑아내, subject_data에 저장\n",
    "        subject_data = [image.get_data() for image in images]\n",
    "        #생덩된 하둡파일에 데이터 추가\n",
    "        add_data_to_storage(data_storage, truth_storage, affine_storage, subject_data, images[0].affine, n_channels,\n",
    "                            truth_dtype)\n",
    "    return data_storage, truth_storage\n",
    "\n",
    "#생성된 하둡파일에 데이터 추가\n",
    "#그냥, 각각 세 list에 각각 추가하는것\n",
    "def add_data_to_storage(data_storage, truth_storage, affine_storage, subject_data, affine, n_channels, truth_dtype):\n",
    "    data_storage.append(np.asarray(subject_data[:n_channels])[np.newaxis])\n",
    "    truth_storage.append(np.asarray(subject_data[n_channels], dtype=truth_dtype)[np.newaxis][np.newaxis])\n",
    "    affine_storage.append(np.asarray(affine)[np.newaxis])\n",
    "\n",
    "\n",
    "#hdf5 파일에 data 기록\n",
    "#학습 이미지 세트를 가져와서 해당 이미지를 hdf5 파일에 기록한다.\n",
    "#함수에 들어있는 파라미터의 의미는 다음과 같다.\n",
    "# training_data_files : 훈련 데이터 파일을 포함하는 튜플 목록. modality(t1, t2, t1ce, flair)은 튜플내에서 다음과 같은 순서로 나열되어야한다.\n",
    "# 마지막 last item은 각 튜플에서 라벨링된 이미지다. 그래서, 1을 빼준다.\n",
    "\n",
    "#outfile : hdf5 파일이 쓰일 경로\n",
    "#image_shape : hdf5파일에 저장될 이미지의 shape\n",
    "#truth_dtype : default = 8-bit 양의 정수로 설정\n",
    "\n",
    "def write_data_to_file(training_data_files, out_file, image_shape, truth_dtype=np.uint8, subject_ids=None,\n",
    "                       normalize=True, crop=False): # J.Lee: crop=True -> crop=False\n",
    "    \"\"\"\n",
    "    Takes in a set of training images and writes those images to an hdf5 file.\n",
    "    :param training_data_files: List of tuples containing the training data files. The modalities should be listed in\n",
    "    the same order in each tuple. The last item in each tuple must be the labeled image. \n",
    "    Example: [('sub1-T1.nii.gz', 'sub1-T2.nii.gz', 'sub1-truth.nii.gz'), \n",
    "              ('sub2-T1.nii.gz', 'sub2-T2.nii.gz', 'sub2-truth.nii.gz')]\n",
    "    :param out_file: Where the hdf5 file will be written to.\n",
    "    :param image_shape: Shape of the images that will be saved to the hdf5 file.\n",
    "    :param truth_dtype: Default is 8-bit unsigned integer.\n",
    "    :return: Location of the hdf5 file with the image data written to it. \n",
    "    \"\"\"\n",
    "    n_samples = len(training_data_files)\n",
    "    n_channels = 4 #SSH: len(training_data_files[0]) - 1, t1, t1ce, t2, flair, label된 이미지[seg], 그래서, 1을 빼준다.\n",
    "    \n",
    "    #이상 없으면, 데이터 파일 생성 함수에 파라미터 값을 넘겨주고, 함수를 실행후, 리턴된 값을, 각각 파일에 저장\n",
    "    try:\n",
    "        hdf5_file, data_storage, truth_storage, affine_storage = create_data_file(out_file,\n",
    "                                                                                  n_channels=n_channels,\n",
    "                                                                                  n_samples=n_samples,\n",
    "                                                                                  image_shape=image_shape)\n",
    "    #예외 발생시, 미완성된 datafile을 삭제, 그래서 예전에 만들어놓은 hdf5파일이 없어진듯\n",
    "    except Exception as e:\n",
    "        # If something goes wrong, delete the incomplete data file\n",
    "        os.remove(out_file)\n",
    "        raise e\n",
    "\n",
    "    #파일에 이미지 데이터 기록\n",
    "    write_image_data_to_file(training_data_files, data_storage, truth_storage, image_shape,\n",
    "                             truth_dtype=truth_dtype, n_channels=n_channels, affine_storage=affine_storage, crop=crop)\n",
    "    #subject_ids값이 들어오면, hdf5_file에 subject_ids라는 \n",
    "    if subject_ids:\n",
    "        hdf5_file.create_array(hdf5_file.root, 'subject_ids', obj=subject_ids)\n",
    "    #normalize값이 들어오면, datastorage 부분을 정규화 시키고, hdf5파일 닫기\n",
    "    if normalize:\n",
    "        normalize_data_storage(data_storage)\n",
    "    hdf5_file.close()\n",
    "    \n",
    "    return out_file\n",
    "\n",
    "#읽기모드로, 테이블파일 열기\n",
    "def open_data_file(filename, readwrite=\"r\"):\n",
    "    return tables.open_file(filename, readwrite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:55:38.127265Z",
     "start_time": "2020-05-28T18:55:38.122264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_11_1/BraTS19_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_11_1/BraTS19_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_11_1/BraTS19_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_11_1/BraTS19_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_11_1/BraTS19_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         0\n",
       "0  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_11_1/BraTS19_...\n",
       "1  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_11_1/BraTS19_...\n",
       "2  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_11_1/BraTS19_...\n",
       "3  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_11_1/BraTS19_...\n",
       "4  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_11_1/BraTS19_..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         0\n",
       "0  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...\n",
       "1  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...\n",
       "2  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...\n",
       "3  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...\n",
       "4  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#modified from N.Freidman's preprocessing code\n",
    "#나오미코드를 참조해서 만든 전처리\n",
    "#우리가 가진 데이터 셋을 분류 및 원하는 순서대로 정렬하고, 트레이닝 데이터 파일에 저장\n",
    "#hdf5파일을 생성 함수와, 생성된 hdf5파일에 데이터셋 기록 및 추가, 읽어오는 함수가 정의\n",
    "\n",
    "data_dir = 'C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/'\n",
    "DATA_HGG = data_dir+'HGG/'\n",
    "DATA_LGG = data_dir+'LGG/'\n",
    "\n",
    "#modality(mri 사진 종류를다음과 같은 순서로 정렬)\n",
    "def modality_sorter(file_path, modality_order = ['flair', 't1','t1ce', 't2', 'seg']):\n",
    "    '''used as key to sort {file path} by {modality_order}'''\n",
    "    for i, modality in enumerate(modality_order):\n",
    "        if modality in file_path: return i\n",
    "\n",
    "#다음과 같이 빈배열을 만들고, BraTS19라는 이름이 경로내에 있으며, nii.gz라는 명을 가지고, nb4라는 명이 없으면, filepaths에 저장 \n",
    "#아까 정의한 modality order대로, 정렬후,hgg_path와 lgg_path에 추가\n",
    "hgg_paths = []\n",
    "for dirpath, dirnames, files in os.walk(DATA_HGG):\n",
    "    if ('BraTS19' in dirpath):\n",
    "        file_paths = [dirpath+'/'+file for file in files if ('nii.gz' in file) and ('nb4' not in file)] # J.Lee: include N4Bias images if you want\n",
    "        file_paths.sort(key=modality_sorter)\n",
    "        hgg_paths.append(tuple(file_paths))\n",
    "\n",
    "lgg_paths = []\n",
    "for dirpath, dirnames, files in os.walk(DATA_LGG):\n",
    "    if ('BraTS19' in dirpath):\n",
    "        file_paths = [dirpath+'/'+file for file in files if ('nii.gz' in file) and ('nb4' not in file)] # J.Lee: include N4Bias images if you want\n",
    "        file_paths.sort(key=modality_sorter)\n",
    "        lgg_paths.append(tuple(file_paths))\n",
    "\n",
    "training_data_files = hgg_paths+lgg_paths\n",
    "\n",
    "#데이타 프레임을 보기 위한 코드\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "pd.set_option('max_colwidth', 120)\n",
    "\n",
    "display(pd.DataFrame(training_data_files[1]))\n",
    "display(pd.DataFrame(training_data_files[-1]))\n",
    "\n",
    "out_file = data_dir+'htf5/'\n",
    "image_shape = (155,240,240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:55:38.132264Z",
     "start_time": "2020-05-28T18:55:38.128266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntraining_data_files = training_data_files\\noutfiles = 'C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/h5_outfiles.h5'\\nimage_shape = (155,240,240)\\nwrite_data_to_file(training_data_files, outfiles, image_shape)\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#위에서 정의한, write_data_to_file을 실행해, 해당 경로에, h5_outfiles.h5 파일 생성, 파일이 존재하면 생략해도 무방\n",
    "'''\n",
    "training_data_files = training_data_files\n",
    "outfiles = 'C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/h5_outfiles.h5'\n",
    "image_shape = (155,240,240)\n",
    "write_data_to_file(training_data_files, outfiles, image_shape)\n",
    "'''\n",
    "# J.Lee comment: write_data_to_file function will be executed in the final step ('main() function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:55:38.295266Z",
     "start_time": "2020-05-28T18:55:38.134266Z"
    }
   },
   "outputs": [],
   "source": [
    "#generator related functions\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "#utils/utils.py\n",
    "#pickle 함수 정의\n",
    "\n",
    "#pickle write\n",
    "def pickle_dump(item, out_file):\n",
    "    with open(out_file, \"wb\") as opened_file:\n",
    "        pickle.dump(item, opened_file)\n",
    "\n",
    "#pickle load\n",
    "def pickle_load(in_file):\n",
    "    with open(in_file, \"rb\") as opened_file:\n",
    "        return pickle.load(opened_file)\n",
    "\n",
    "#utils/patches.py\n",
    "# import numpy as np\n",
    "#patch 인덱스 연산 및, 코너나 경계값에서, patch를 어떻게 처리할지에 대해 정의 된 함수가 들어있다.\n",
    "\n",
    "#patch 인덱스 연산함수(image_shape, patch_size, overlap, start=None)\n",
    "#image_shape : (155, 240, 240)\n",
    "#patch_shape : 이미지 shape를 탐색하면서 훑어볼 작은 상자\n",
    "#overlap : patch간의 겹치는부분\n",
    "def compute_patch_indices(image_shape, patch_size, overlap, start=None):\n",
    "    #overlap 변수가 int형이면, 실행하여,overlap값을 결정 \n",
    "    if isinstance(overlap, int):\n",
    "        #overlap = np.asarray([overlap]*len[155, 240, 240]), 다음과 같이 연산하여, 겹치는 부분을 결정\n",
    "        overlap = np.asarray([overlap] * len(image_shape))\n",
    "        \n",
    "    #start가 None이면, 실행\n",
    "    if start is None:\n",
    "        #(image_shape/patch_size-overlap) 연산 후 올림하여, n_patches에 저장\n",
    "        #즉, 패치들의 개수 = 전체 3D이미지/patch들간의 합집합 연산 후 반올림\n",
    "        #그냥, patch들의 수 구하는 공식\n",
    "        n_patches = np.ceil(image_shape / (patch_size - overlap))\n",
    "        #(patch_size - overlap) * n_patches - image_shape + overlap 연산 후 overflow에 저장\n",
    "        #즉, overflow는 patch들간의 겹치지 않은 부분들간의 집합 + 겹치는 부분들의 집합 - image 전체 모양\n",
    "        overflow = (patch_size - overlap) * n_patches - image_shape + overlap\n",
    "        #overflow/2한 값을 반올림 한후 -부호를 붙여, start 값으로 지정\n",
    "        start = -np.ceil(overflow/2)\n",
    "    #start 값이 정상적으로 정수값으로 존재하면\n",
    "    elif isinstance(start, int):\n",
    "        #다음과 같이 연산하여, 시작지점을 결정\n",
    "        start = np.asarray([start] * len(image_shape))\n",
    "    #stop은 탐색 종료부분\n",
    "    stop = image_shape + start\n",
    "    #step은 patch로 전체 이미지를 훑는데, 몇단계가 걸리느냐?\n",
    "    step = patch_size - overlap\n",
    "    #구한, start, stop, step함수로 get_set_of_patch_indices함수를 호출\n",
    "    return get_set_of_patch_indices(start, stop, step)\n",
    "\n",
    "#start, stop, step 값을 받아, 각 axis별로, 값을 세팅한 후에, 정수로 되어있는 전치행렬로 만든후, np.mgird로 차원뻥튀기를 해준다.\n",
    "#만약, start가 0, stop이 10이고, step이 5면, 0, 2, 4, 6, 8값을 가지고, step이 2이면, 0, 5 이런 값을 가진다.\n",
    "#step값 미지정시, 0, 1, 2, 3 .... 9까지 증가한다.\n",
    "def get_set_of_patch_indices(start, stop, step):\n",
    "    return np.asarray(np.mgrid[start[0]:stop[0]:step[0], start[1]:stop[1]:step[1],\n",
    "                               start[2]:stop[2]:step[2]].reshape(3, -1).T, dtype=np.int)\n",
    "\n",
    "#patch index를 임의로 얻어온다.\n",
    "def get_random_patch_index(image_shape, patch_shape):\n",
    "    #patch의 코너 인덱스를 랜덤하게 반환한다. 만약, training중 이 함수를 사용하면, 중간 픽셀이 가장자리 픽셀보다 더 자주 보이게 될것이다.\n",
    "    #이건 나쁜 일이니, 트레이닝 중에 사용하지 말라는것 같다.\n",
    "    \"\"\"\n",
    "    Returns a random corner index for a patch. If this is used during training, the middle pixels will be seen by\n",
    "    the model way more often than the edge pixels (which is probably a bad thing).\n",
    "    :param image_shape: Shape of the image\n",
    "    :param patch_shape: Shape of the patch\n",
    "    :return: a tuple containing the corner index which can be used to get a patch from an image\n",
    "    \"\"\"\n",
    "    #image_shape와 patch_shape의 차를 구한후 랜덤하게 반환한다.\n",
    "    return get_random_nd_index(np.subtract(image_shape, patch_shape))\n",
    "\n",
    "#nd index를 임의로 얻어온다.\n",
    "def get_random_nd_index(index_max):\n",
    "    return tuple([np.random.choice(index_max[index] + 1) for index in range(len(index_max))])\n",
    "\n",
    "#3d 데이터에서 patch를 얻어온다.\n",
    "\n",
    "#data : patch에서 얻어온 numpy array\n",
    "#patch_shape : patch의 모양/사이즈\n",
    "#patch_index : patch의 코너인덱스\n",
    "def get_patch_from_3d_data(data, patch_shape, patch_index):\n",
    "    \"\"\"\n",
    "    Returns a patch from a numpy array.\n",
    "    :param data: numpy array from which to get the patch.\n",
    "    :param patch_shape: shape/size of the patch.\n",
    "    :param patch_index: corner index of the patch.\n",
    "    :return: numpy array take from the data with the patch shape specified.\n",
    "    \"\"\"\n",
    "    patch_index = np.asarray(patch_index, dtype=np.int16)\n",
    "    patch_shape = np.asarray(patch_shape)\n",
    "    image_shape = data.shape[-3:]\n",
    "    \n",
    "    #np.any : np배열 내부에서 조건에 맞는 데이터가 있으면 True, 아니면, False 반환\n",
    "    #즉, patch가 image의 범위에서 벗어난 부분에 대해서, 보정을 해주는 부분인듯, 즉, 작은 patch라는 상자가, 큰 상자의 바깥부분에\n",
    "    #존재할때, 그 값들을 보정하기 위하여, fix_out_of_bound_patch_attempt함수를 불러와서, 보정해주는것인듯, 그 외에는 그냥 리턴\n",
    "    if np.any(patch_index < 0) or np.any((patch_index + patch_shape) > image_shape):\n",
    "        data, patch_index = fix_out_of_bound_patch_attempt(data, patch_shape, patch_index)\n",
    "    return data[..., patch_index[0]:patch_index[0]+patch_shape[0], patch_index[1]:patch_index[1]+patch_shape[1],\n",
    "                patch_index[2]:patch_index[2]+patch_shape[2]]\n",
    "\n",
    "#경계부분 patch 수정, 이 함수는 위의, 3d_data로 부터, patch를 얻어올때만 쓰인다.\n",
    "def fix_out_of_bound_patch_attempt(data, patch_shape, patch_index, ndim=3):\n",
    "    \"\"\"\n",
    "    Pads the data and alters the patch index so that a patch will be correct.\n",
    "    :param data:\n",
    "    :param patch_shape:\n",
    "    :param patch_index:\n",
    "    :return: padded data, fixed patch index\n",
    "    \"\"\"\n",
    "    #image_shape = data.shape\n",
    "    image_shape = data.shape[-ndim:]\n",
    "    #pad_before = patch가 음수면, 제곱해서, 절대값으로 변환\n",
    "    pad_before = np.abs((patch_index < 0) * patch_index)\n",
    "    #pad_after = patch가 image_shape를 이탈하면, 다음 방법으로 값을 보정\n",
    "    pad_after = np.abs(((patch_index + patch_shape) > image_shape) * ((patch_index + patch_shape) - image_shape))\n",
    "    #pad_before, pad_after를 axis = 1로 병합\n",
    "    pad_args = np.stack([pad_before, pad_after], axis=1)\n",
    "    #만약, pad_args의 차원이 \n",
    "    #tolist는 nd array를 list로 바꾸어주는 함수\n",
    "    if pad_args.shape[0] < len(data.shape):\n",
    "        pad_args = [[0, 0]] * (len(data.shape) - pad_args.shape[0]) + pad_args.tolist()\n",
    "    #np.pad(data, padding array, edge부분으로 처리)\n",
    "    #즉, 데이터가 존재하고, padding에 edge라고 처리하는것이다.\n",
    "    #예를 들어, data = [[1,2],[3,4]], pad_args = [[2, 2], [2, 2]], mode = \"edge\"면\n",
    "    '''\n",
    "    e e e e e e\n",
    "    e e e e e e\n",
    "    e e 1 2 e e\n",
    "    e e 3 4 e e\n",
    "    e e e e e e\n",
    "    e e e e e e\n",
    "    \n",
    "    e = edge모드로 처리될 부분, 이런식으로 처리된다는 의미, 즉, 전체 이미지의 가장자리 부분을 처리하는 부분이다.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    data = np.pad(data, pad_args, mode=\"edge\")\n",
    "    patch_index += pad_before\n",
    "    #data와 patch index 반환\n",
    "    return data, patch_index\n",
    "\n",
    "#patch 목록 및 해당 patch index에서 원래 모양의 배열을 재구성합니다. 겹치는 패치는 평균화\n",
    "\n",
    "#patch들로부터 재설계\n",
    "#patches : numpy array로 된 patch들의 목록\n",
    "#patch_indicces : patch들의 목록에 해당하는 index 목록\n",
    "#patch들로부터 추출한 array의 shape\n",
    "#default값 : 결과데이터의 기본값, 만약, patch 적용이 완료되면, 이 값은 덮어씌어진다.\n",
    "\n",
    "\n",
    "#함수는 정의는 되어있는데, 사용된곳이 없어서 보류\n",
    "def reconstruct_from_patches(patches, patch_indices, data_shape, default_value=0):\n",
    "    \"\"\"\n",
    "    Reconstructs an array of the original shape from the lists of patches and corresponding patch indices. Overlapping\n",
    "    patches are averaged.\n",
    "    :param patches: List of numpy array patches.\n",
    "    :param patch_indices: List of indices that corresponds to the list of patches.\n",
    "    :param data_shape: Shape of the array from which the patches were extracted.\n",
    "    :param default_value: The default value of the resulting data. if the patch coverage is complete, this value will\n",
    "    be overwritten.\n",
    "    :return: numpy array containing the data reconstructed by the patches.\n",
    "    \"\"\"\n",
    "    data = np.ones(data_shape) * default_value\n",
    "    image_shape = data_shape[-3:]\n",
    "    count = np.zeros(data_shape, dtype=np.int)\n",
    "    for patch, index in zip(patches, patch_indices):\n",
    "        image_patch_shape = patch.shape[-3:]\n",
    "        if np.any(index < 0):\n",
    "            fix_patch = np.asarray((index < 0) * np.abs(index), dtype=np.int)\n",
    "            patch = patch[..., fix_patch[0]:, fix_patch[1]:, fix_patch[2]:]\n",
    "            index[index < 0] = 0\n",
    "        if np.any((index + image_patch_shape) >= image_shape):\n",
    "            fix_patch = np.asarray(image_patch_shape - (((index + image_patch_shape) >= image_shape)\n",
    "                                                        * ((index + image_patch_shape) - image_shape)), dtype=np.int)\n",
    "            patch = patch[..., :fix_patch[0], :fix_patch[1], :fix_patch[2]]\n",
    "        patch_index = np.zeros(data_shape, dtype=np.bool)\n",
    "        patch_index[...,\n",
    "                    index[0]:index[0]+patch.shape[-3],\n",
    "                    index[1]:index[1]+patch.shape[-2],\n",
    "                    index[2]:index[2]+patch.shape[-1]] = True\n",
    "        patch_data = np.zeros(data_shape)\n",
    "        patch_data[patch_index] = patch.flatten()\n",
    "\n",
    "        new_data_index = np.logical_and(patch_index, np.logical_not(count > 0))\n",
    "        data[new_data_index] = patch_data[new_data_index]\n",
    "\n",
    "        averaged_data_index = np.logical_and(patch_index, count > 0)\n",
    "        if np.any(averaged_data_index):\n",
    "            data[averaged_data_index] = (data[averaged_data_index] * count[averaged_data_index] + patch_data[averaged_data_index]) / (count[averaged_data_index] + 1)\n",
    "        count[patch_index] += 1\n",
    "    return data\n",
    "\n",
    "\n",
    "#######################\n",
    "#augmentaion에 사용된 함수들, scale 변경, 뒤집기, transpose등과 관련된 함수들의 정의되어 있다.\n",
    "#augment.py\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn.image import new_img_like, resample_to_img\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "#이미지에 크기변환 후, affine 적용하는 함수\n",
    "#new_img_like : 참조 이미지와 동일한 새로운 이미지를 만드는데 사용하는 함수, (이미지, data, affine 배열행렬)\n",
    "def scale_image(image, scale_factor):\n",
    "    scale_factor = np.asarray(scale_factor)\n",
    "    new_affine = np.copy(image.affine)\n",
    "    new_affine[:3, :3] = image.affine[:3, :3] * scale_factor\n",
    "    new_affine[:, 3][:3] = image.affine[:, 3][:3] + (image.shape * np.diag(image.affine)[:3] * (1 - scale_factor)) / 2\n",
    "    return new_img_like(image, data=image.get_data(), affine=new_affine)\n",
    "\n",
    "\n",
    "#이미지와 축을 이용해, axis를 기준으로, 반전해주는 함수\n",
    "def flip_image(image, axis):\n",
    "    try:\n",
    "        new_data = np.copy(image.get_data())\n",
    "        for axis_index in axis:\n",
    "            new_data = np.flip(new_data, axis=axis_index)\n",
    "    except TypeError:\n",
    "        new_data = np.flip(image.get_data(), axis=axis)\n",
    "    return new_img_like(image, data=new_data)\n",
    "\n",
    "#아래의 augmentation data 함수에 사용하기 위해, 정의된 함수\n",
    "#len(truth.shape)값을 인자로 받게됨\n",
    "def random_flip_dimensions(n_dimensions):\n",
    "    axis = list()\n",
    "    for dim in range(n_dimensions):\n",
    "        #random_boolean을 형성한다. True or False의 값을 뱉어내게 되는데, 1이면, axis에 axis에 값을 추가한다.\n",
    "        if random_boolean():\n",
    "            axis.append(dim)\n",
    "    #결과적으로, (truth.shape의 factor들을 무작위로 axis에 집어 넣게 된다.)\n",
    "    return axis\n",
    "\n",
    "#얘도, augmentation data 함수에 사용하기 위해, 정의된 함수\n",
    "#무작위 표본 정규분표를 만든다. np.random.normal(location, size, scale)\n",
    "def random_scale_factor(n_dim=3, mean=1, std=0.25):\n",
    "    return np.random.normal(mean, std, n_dim)\n",
    "\n",
    "#위의 random_flip_dimensions에 사용하기 위해 정의된 함수\n",
    "def random_boolean():\n",
    "    return np.random.choice([True, False])\n",
    "\n",
    "#얘도, augmentation data 함수에 사용하기 위해, 정의된 함수\n",
    "#distort_image(이미지, 반전 기준 축, scale_factor), random으로 flip_axis에 append 했기 때문에, 값이 None일 수도 있고, 있을 수도 있다.\n",
    "#flip_axis 값이 있다면, 해당 axis를 기준으로 반전\n",
    "#scale_factor도 마찬가지\n",
    "#이 둘을 적용시켜, 왜곡된 이미지를 뱉어낸다.\n",
    "def distort_image(image, flip_axis=None, scale_factor=None):\n",
    "    if flip_axis:\n",
    "        image = flip_image(image, flip_axis)\n",
    "    if scale_factor is not None:\n",
    "        image = scale_image(image, scale_factor)\n",
    "    return image\n",
    "\n",
    "#위의 정의된 함수들을 다 이 함수에 적용시켜서, 데이터를 augmentation 하는 함수\n",
    "def augment_data(data, truth, affine, scale_deviation=None, flip=True):\n",
    "    n_dim = len(truth.shape)\n",
    "    if scale_deviation:\n",
    "        scale_factor = random_scale_factor(n_dim, std=scale_deviation)\n",
    "    else:\n",
    "        scale_factor = None\n",
    "    if flip:\n",
    "        flip_axis = random_flip_dimensions(n_dim)\n",
    "    else:\n",
    "        flip_axis = None\n",
    "    data_list = list()\n",
    "    \n",
    "    for data_index in range(data.shape[0]):\n",
    "        image = get_image(data[data_index], affine)\n",
    "        #datalist에 왜곡을 시킨 이미지를 재조정후에 저장, resample_to_image함수에 대한 설명은 아래에\n",
    "        #resample_to_img(source_img, target_img, interpolation = 'continuous',copy = True ,order = 'F' ,clip = False ,fill_value = 0 , force_resample = False )\n",
    "        data_list.append(resample_to_img(distort_image(image, flip_axis=flip_axis,\n",
    "                                                       scale_factor=scale_factor), image,\n",
    "                                         interpolation=\"continuous\").get_data())\n",
    "    #아래도 같음\n",
    "    data = np.asarray(data_list)\n",
    "    truth_image = get_image(truth, affine)\n",
    "    truth_data = resample_to_img(distort_image(truth_image, flip_axis=flip_axis, scale_factor=scale_factor),\n",
    "                                 truth_image, interpolation=\"nearest\").get_data()\n",
    "    return data, truth_data\n",
    "\n",
    "\n",
    "#nib.Nifti1Image(new_data, img.affine, img.header[얘는 선택사항, 없어도 됨])\n",
    "#이미지에 affine 적용후, 새로운 이미지를 생성하는 함수, 위의 augment_data함수에서 사용됨\n",
    "def get_image(data, affine, nib_class=nib.Nifti1Image):\n",
    "    return nib_class(dataobj=data, affine=affine)\n",
    "\n",
    "\n",
    "#여기는 만들어놓고 안쓴 함수들, 3D를 회전시키고, 뒤집고 하는데 사용한 함수들 정의\n",
    "\n",
    "\n",
    "#이 함수는 48 개의 고유한 회전을 나타내는 \"키\"세트를 반환합니다. 즉, 3D 회전 매트릭스를 반환\n",
    "#각 세트의 항목은 튜플로 되어있다. ((rotate_y, rotate_z), flip_x, flip_y, flip_z, transpose)\n",
    "#예를들어 ((0, 1), 0, 1, 0, 1) 이것은, z축을 중심으로 90도 회전후, y축 기준으로 뒤집고, transpose 시킨다.\n",
    "def generate_permutation_keys():\n",
    "    \"\"\"\n",
    "    This function returns a set of \"keys\" that represent the 48 unique rotations &\n",
    "    reflections of a 3D matrix.\n",
    "\n",
    "    Each item of the set is a tuple:\n",
    "    ((rotate_y, rotate_z), flip_x, flip_y, flip_z, transpose)\n",
    "\n",
    "    As an example, ((0, 1), 0, 1, 0, 1) represents a permutation in which the data is\n",
    "    rotated 90 degrees around the z-axis, then reversed on the y-axis, and then\n",
    "    transposed.\n",
    "\n",
    "    48 unique rotations & reflections:\n",
    "    https://en.wikipedia.org/wiki/Octahedral_symmetry#The_isometries_of_the_cube\n",
    "    \"\"\"\n",
    "    #집합을 반환\n",
    "    #itertools.product는 데카르트 곱을 의미\n",
    "    #즉, itertools.combinations_with_replacement(range(2), 2) 와 range(2), range(2), range(2), range(2), range(2)간에 일어날 수 있는\n",
    "    #모든 순열을 생성\n",
    "    #원래는 itertools.product( ,repeat=n) 값을 정해주어야 하나, repeat 값이 없으므로, 그냥 1\n",
    "    #즉 여기서는, 그냥,나열한것\n",
    "    \n",
    "    #ex)itertools.product(\"ABC\", 2)\n",
    "    #>>> AA, AB, AC, BA, BB, BC, CA, CB, CC\n",
    "    \n",
    "    #itertools.combinations_with_replacement은, itertools.product에서 AB와 BA를 같은것으로 보는것, 조합임\n",
    "    \n",
    "    #ex)itertools.combinations_with_replacement('ABC', 2)\n",
    "    #>>> AA AB AC BB BC CC\n",
    "\n",
    "    #즉, 얘를 풀면\n",
    "    #itertools.combinations_with_replacement(range(2), 2) = 00, 01, 11 3가지\n",
    "    #itertools.product([00, 01, 11], [0, 1], [0, 1], [0, 1], [0, 1])\n",
    "    #[]에서 하나씩 추출해서, 순열을 만든다. 그러면, 3*2*2*2*2 = 48가지가 나온다.\n",
    "    \n",
    "    return set(itertools.product(\n",
    "        itertools.combinations_with_replacement(range(2), 2), range(2), range(2), range(2), range(2)))\n",
    "\n",
    "#위의 list(generate_permutation_keys())에서 생성된 48개중 하나를 골라 반환\n",
    "def random_permutation_key():\n",
    "    \"\"\"\n",
    "    Generates and randomly selects a permutation key. See the documentation for the\n",
    "    \"generate_permutation_keys\" function.\n",
    "    \"\"\"\n",
    "    return random.choice(list(generate_permutation_keys()))\n",
    "\n",
    "\n",
    "#주어진 키의 특성에 따라 데이터를 바꿔준다. 인풋데이터 모양은 반드시, 다음 형태를 가져야 한다.(n_modalities, x, y, z)\n",
    "#인풋 키는 튜플로 다음과 같이 구성되어 있다. Input key is a tuple: (rotate_y, rotate_z), flip_x, flip_y, flip_z, transpose)\n",
    "#예를 들어, ((0, 1), 0, 1, 0, 1) 이것은, z축으로 90도 회전하고, y축 기준으로 반전시키고, transpose한것이다.\n",
    "\n",
    "def permute_data(data, key):\n",
    "    \"\"\"\n",
    "    Permutes the given data according to the specification of the given key. Input data\n",
    "    must be of shape (n_modalities, x, y, z).\n",
    "\n",
    "    Input key is a tuple: (rotate_y, rotate_z), flip_x, flip_y, flip_z, transpose)\n",
    "\n",
    "    As an example, ((0, 1), 0, 1, 0, 1) represents a permutation in which the data is\n",
    "    rotated 90 degrees around the z-axis, then reversed on the y-axis, and then\n",
    "    transposed.\n",
    "    \"\"\"\n",
    "    \n",
    "    #아래는, 튜플의 값에 따라, daata를 바꿔주는것을 구현한 코드\n",
    "    data = np.copy(data)\n",
    "    (rotate_y, rotate_z), flip_x, flip_y, flip_z, transpose = key\n",
    "\n",
    "    if rotate_y != 0:\n",
    "        data = np.rot90(data, rotate_y, axes=(1, 3))\n",
    "    if rotate_z != 0:\n",
    "        data = np.rot90(data, rotate_z, axes=(2, 3))\n",
    "    if flip_x:\n",
    "        data = data[:, ::-1]\n",
    "    if flip_y:\n",
    "        data = data[:, :, ::-1]\n",
    "    if flip_z:\n",
    "        data = data[:, :, :, ::-1]\n",
    "    if transpose:\n",
    "        for i in range(data.shape[0]):\n",
    "            data[i] = data[i].T\n",
    "    return data\n",
    "\n",
    "#위에서 정의한, random_permutation_key() 값인 키값을 받아, permute_data 함수를 수행하여, 데이터 변환\n",
    "#들어오는 데이터는 반드시, (n_modalities, x, y, z)의 형태를 가진, numpy array여야 할것\n",
    "def random_permutation_x_y(x_data, y_data):\n",
    "    \"\"\"\n",
    "    Performs random permutation on the data.\n",
    "    :param x_data: numpy array containing the data. Data must be of shape (n_modalities, x, y, z).\n",
    "    :param y_data: numpy array containing the data. Data must be of shape (n_modalities, x, y, z).\n",
    "    :return: the permuted data\n",
    "    \"\"\"\n",
    "    key = random_permutation_key()\n",
    "    return permute_data(x_data, key), permute_data(y_data, key)\n",
    "\n",
    "#permute_data(data, key) 함수를 적용하기 전으로 돌아감 \n",
    "def reverse_permute_data(data, key):\n",
    "    key = reverse_permutation_key(key)\n",
    "    data = np.copy(data)\n",
    "    (rotate_y, rotate_z), flip_x, flip_y, flip_z, transpose = key\n",
    "\n",
    "    if transpose:\n",
    "        for i in range(data.shape[0]):\n",
    "            data[i] = data[i].T\n",
    "    if flip_z:\n",
    "        data = data[:, :, :, ::-1]\n",
    "    if flip_y:\n",
    "        data = data[:, :, ::-1]\n",
    "    if flip_x:\n",
    "        data = data[:, ::-1]\n",
    "    if rotate_z != 0:\n",
    "        data = np.rot90(data, rotate_z, axes=(2, 3))\n",
    "    if rotate_y != 0:\n",
    "        data = np.rot90(data, rotate_y, axes=(1, 3))\n",
    "    return data\n",
    "\n",
    "#random_permutation_key()를 적용하기 전 상황으로 복구하는 키를 만드는 함수\n",
    "def reverse_permutation_key(key):\n",
    "    rotation = tuple([-rotate for rotate in key[0]])\n",
    "    return rotation, key[1], key[2], key[3], key[4]\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "#generator관련 함수가 정의되있다.\n",
    "\n",
    "#generator.py\n",
    "import os\n",
    "import copy\n",
    "from random import shuffle\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from .utils import pickle_dump, pickle_load\n",
    "# from .utils.patches import compute_patch_indices, get_random_nd_index, get_patch_from_3d_data\n",
    "# from .augment import augment_data, random_permutation_x_y\n",
    "\n",
    "#training 과 validation generator 정의, 모델을 트레이닝 할때 사용\n",
    "\n",
    "#data_file : 트레이닝에 사용할 data_file\n",
    "#batch_size : 트레이닝 batchsize\n",
    "#n_labels : 이진화된 레이블 수\n",
    "#training_keys_file : 훈련 데이터의 인덱스 위치가 기록된 picklefile\n",
    "#validation_keys_file : 검증 데이터의 인덱스 위치가 기록된 picklefile\n",
    "\n",
    "#data_split : 훈련 데이터와 검증데이터 분리, 0.8 -> 훈련데이터 : 검증데이터 = 8:2\n",
    "#overwrite : 덮어쓰기\n",
    "#labels = 이미지 파일들 내부에서 정렬된 라벨값들을 가진, 리스트나 튜플이다. 리스트 또는 튜플의 길이는 n_labels의 값과 같다.\n",
    "#augment = 만약, True로 하면, 트레이닝 이미지가 오버피팅을 방지하기 위해, 왜곡된다.\n",
    "\n",
    "#augment_flip : 만약, augment 값이 True고, 얘도 True이면, 무작위로, x, y, z축으로 반전된다.\n",
    "#augment_distortion_factor : 만약, augment 값이 True이면, 원본과 왜곡된 데이터값의 표준편차를 계산한다.(늘어나거나, 수축된 정도), None으로\n",
    "#하거나, Farse 또는 0으로 하면, 이 방식으로, 데이터가 왜곡되지 않는다.\n",
    "#patch_shape : generator와 함께 반환할 데이터 shape, 만약, None값이면, 전체 이미지 shape가 반환\n",
    "\n",
    "#validation_patch_overlap : 검증 데이터에서 겹쳐질 픽셀 / 복셀 수(필요한 patch_shape는 None이 아님)\n",
    "#training_patch_start_offset : 정수값이 들어있는 길이 3의 튜플로, 트레이닝 데이터가 무작위로 (0, 0, 0) ~ (training_patch_start_offset) \n",
    "#사이의 픽셀 수 만큼 오프셋된다.\n",
    "\n",
    "#validation_batch_size : 검증 데이터의 batchsize\n",
    "#skip_blank : 만약 True면, 이미지 또는 패치에서 0값을 가진부분은 data generator에서 생략한다.\n",
    "#permute : 무작위로 데이터를 치환, 데이터는 반드시 3차원 큐브형태여야한다.\n",
    "\n",
    "\n",
    "def get_training_and_validation_generators(data_file, batch_size, n_labels, training_keys_file, validation_keys_file,\n",
    "                                           data_split=0.8, overwrite=False, labels=None, augment=False,\n",
    "                                           augment_flip=True, augment_distortion_factor=0.25, patch_shape=None,\n",
    "                                           validation_patch_overlap=0, training_patch_start_offset=None,\n",
    "                                           validation_batch_size=None, skip_blank=True, permute=False):\n",
    "    \"\"\"\n",
    "    Creates the training and validation generators that can be used when training the model.\n",
    "    :param skip_blank: If True, any blank (all-zero) label images/patches will be skipped by the data generator.\n",
    "    :param validation_batch_size: Batch size for the validation data.\n",
    "    :param training_patch_start_offset: Tuple of length 3 containing integer values. Training data will randomly be\n",
    "    offset by a number of pixels between (0, 0, 0) and the given tuple. (default is None)\n",
    "    :param validation_patch_overlap: Number of pixels/voxels that will be overlapped in the validation data. (requires\n",
    "    patch_shape to not be None)\n",
    "    :param patch_shape: Shape of the data to return with the generator. If None, the whole image will be returned.\n",
    "    (default is None)\n",
    "    :param augment_flip: if True and augment is True, then the data will be randomly flipped along the x, y and z axis\n",
    "    :param augment_distortion_factor: if augment is True, this determines the standard deviation from the original\n",
    "    that the data will be distorted (in a stretching or shrinking fashion). Set to None, False, or 0 to prevent the\n",
    "    augmentation from distorting the data in this way.\n",
    "    :param augment: If True, training data will be distorted on the fly so as to avoid over-fitting.\n",
    "    :param labels: List or tuple containing the ordered label values in the image files. The length of the list or tuple\n",
    "    should be equal to the n_labels value.\n",
    "    Example: (10, 25, 50)\n",
    "    The data generator would then return binary truth arrays representing the labels 10, 25, and 30 in that order.\n",
    "    :param data_file: hdf5 file to load the data from.\n",
    "    :param batch_size: Size of the batches that the training generator will provide.\n",
    "    :param n_labels: Number of binary labels.\n",
    "    :param training_keys_file: Pickle file where the index locations of the training data will be stored.\n",
    "    :param validation_keys_file: Pickle file where the index locations of the validation data will be stored.\n",
    "    :param data_split: How the training and validation data will be split. 0 means all the data will be used for\n",
    "    validation and none of it will be used for training. 1 means that all the data will be used for training and none\n",
    "    will be used for validation. Default is 0.8 or 80%.\n",
    "    :param overwrite: If set to True, previous files will be overwritten. The default mode is false, so that the\n",
    "    training and validation splits won't be overwritten when rerunning model training.\n",
    "    :param permute: will randomly permute the data (data must be 3D cube)\n",
    "    :return: Training data generator, validation data generator, number of training steps, number of validation steps\n",
    "    \"\"\"\n",
    "    #만약, 검증 데이터의 배치사이즈가 지정되지 않으면, validation_batch_size = batch_size\n",
    "    if not validation_batch_size:\n",
    "        validation_batch_size = batch_size\n",
    "        \n",
    "    #training_list와 validation_list로 분리\n",
    "    training_list, validation_list = get_validation_split(data_file,\n",
    "                                                          data_split=data_split,\n",
    "                                                          overwrite=overwrite,\n",
    "                                                          training_file=training_keys_file,\n",
    "                                                          validation_file=validation_keys_file)\n",
    "\n",
    "    #training genrator 생성\n",
    "    training_generator = data_generator(data_file, training_list,\n",
    "                                        batch_size=batch_size,\n",
    "                                        n_labels=n_labels,\n",
    "                                        labels=labels,\n",
    "                                        augment=augment,\n",
    "                                        augment_flip=augment_flip,\n",
    "                                        augment_distortion_factor=augment_distortion_factor,\n",
    "                                        patch_shape=patch_shape,\n",
    "                                        patch_overlap=0,\n",
    "                                        patch_start_offset=training_patch_start_offset,\n",
    "                                        skip_blank=skip_blank,\n",
    "                                        permute=permute)\n",
    "    \n",
    "    #validation generator 생성\n",
    "    validation_generator = data_generator(data_file, validation_list,\n",
    "                                          batch_size=validation_batch_size,\n",
    "                                          n_labels=n_labels,\n",
    "                                          labels=labels,\n",
    "                                          patch_shape=patch_shape,\n",
    "                                          patch_overlap=validation_patch_overlap,\n",
    "                                          skip_blank=skip_blank)\n",
    "\n",
    "    # Set the number of training and testing samples per epoch correctly\n",
    "    # 에포크 당 트레이닝 및 테스트 샘플 수를 설정\n",
    "    num_training_steps = get_number_of_steps(get_number_of_patches(data_file, training_list, patch_shape,\n",
    "                                                                   skip_blank=skip_blank,\n",
    "                                                                   patch_start_offset=training_patch_start_offset,\n",
    "                                                                   patch_overlap=0), batch_size)\n",
    "    print(\"Number of training steps: \", num_training_steps)\n",
    "\n",
    "    num_validation_steps = get_number_of_steps(get_number_of_patches(data_file, validation_list, patch_shape,\n",
    "                                                                     skip_blank=skip_blank,\n",
    "                                                                     patch_overlap=validation_patch_overlap),\n",
    "                                               validation_batch_size)\n",
    "    print(\"Number of validation steps: \", num_validation_steps)\n",
    "\n",
    "    return training_generator, validation_generator, num_training_steps, num_validation_steps\n",
    "\n",
    "\n",
    "#위의 generator 생성 함수에서 사용, step 수를 구하는 함수\n",
    "def get_number_of_steps(n_samples, batch_size):\n",
    "    if n_samples <= batch_size:\n",
    "        return n_samples\n",
    "    elif np.remainder(n_samples, batch_size) == 0:\n",
    "        return n_samples//batch_size\n",
    "    else:\n",
    "        return n_samples//batch_size + 1\n",
    "\n",
    "#위의 generator 생성 함수에서 사용, training data와 validation data를 분리하는데 사용하는 함수\n",
    "def get_validation_split(data_file, training_file, validation_file, data_split=0.8, overwrite=False):\n",
    "    \"\"\"\n",
    "    Splits the data into the training andc validation indices list.\n",
    "    :param data_file: pytables hdf5 data file\n",
    "    :param training_file:\n",
    "    :param validation_file:\n",
    "    :param data_split:\n",
    "    :param overwrite:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #isensee_training_ids.plk 파일이 없으면 raining과 validation을 분리해서, 저장 후 반환, 있으면, 존재한, trainig_list, validation_list를\n",
    "    #반환\n",
    "    if overwrite or not os.path.exists(training_file):\n",
    "        print(\"Creating validation split...\")\n",
    "        nb_samples = data_file.root.data.shape[0]\n",
    "        sample_list = list(range(nb_samples))\n",
    "        training_list, validation_list = split_list(sample_list, split=data_split)\n",
    "        pickle_dump(training_list, training_file)\n",
    "        pickle_dump(validation_list, validation_file)\n",
    "        return training_list, validation_list\n",
    "    else:\n",
    "        print(\"Loading previous validation split...\")\n",
    "        return pickle_load(training_file), pickle_load(validation_file)\n",
    "\n",
    "#위의 get_validation_split함수에서 사용, 트레이닝셋과 테스팅셋으로 분리하는데 사용\n",
    "def split_list(input_list, split=0.8, shuffle_list=True):\n",
    "    if shuffle_list:\n",
    "        shuffle(input_list)\n",
    "    n_training = int(len(input_list) * split)\n",
    "    training = input_list[:n_training]\n",
    "    testing = input_list[n_training:]\n",
    "    return training, testing\n",
    "\n",
    "#data generator 생성함수, 파라미터에 대한 설명은 위의 training, validation generator와 같다.\n",
    "def data_generator(data_file, index_list, batch_size=1, n_labels=1, labels=None, augment=False, augment_flip=True,\n",
    "                   augment_distortion_factor=0.25, patch_shape=None, patch_overlap=0, patch_start_offset=None,\n",
    "                   shuffle_index_list=True, skip_blank=True, permute=False):\n",
    "    orig_index_list = index_list\n",
    "    while True:\n",
    "        x_list = list()\n",
    "        y_list = list()\n",
    "        #patch_shape 값이 들어오면, patch의 인덱스 list를 생성하는 함수를 실행하고, patch index를 index_list값에 저장\n",
    "        if patch_shape:\n",
    "            index_list = create_patch_index_list(orig_index_list, data_file.root.data.shape[-3:], patch_shape,\n",
    "                                                 patch_overlap, patch_start_offset)\n",
    "        #patch_shape 값이 들어오지 않으면, 그냥, 들어온 값을 그대로 사용\n",
    "        else:\n",
    "            index_list = copy.copy(orig_index_list)\n",
    "        #shuffle_index_list가 참이면, 항목을 섞어준다.\n",
    "        if shuffle_index_list:\n",
    "            shuffle(index_list)\n",
    "        #index_list에 값이 존재하는동안\n",
    "        while len(index_list) > 0:\n",
    "            #맨 마지막 인덱스부터, 하나씩 빼서, index에 저장\n",
    "            index = index_list.pop()\n",
    "            #데이터 파일의 데이터를 feature과 target data 리스트들에 추가하는 함수\n",
    "            add_data(x_list, y_list, data_file, index, augment=augment, augment_flip=augment_flip,\n",
    "                     augment_distortion_factor=augment_distortion_factor, patch_shape=patch_shape,\n",
    "                     skip_blank=skip_blank, permute=permute)\n",
    "            if len(x_list) == batch_size or (len(index_list) == 0 and len(x_list) > 0):\n",
    "                #yield는 generator를 반환한다는것 빼고, return과 비슷\n",
    "                yield convert_data(x_list, y_list, n_labels=n_labels, labels=labels)\n",
    "                x_list = list()\n",
    "                y_list = list()\n",
    "\n",
    "#패치들의 개수를 얻는다.\n",
    "def get_number_of_patches(data_file, index_list, patch_shape=None, patch_overlap=0, patch_start_offset=None,\n",
    "                          skip_blank=True):\n",
    "    #만약, patch_shape값이 존재하면\n",
    "    if patch_shape:\n",
    "        #patch의 인덱스 list를 생성하는 함수를 실행하고 반환된, patch_index를 index_list에 넣는다.\n",
    "        index_list = create_patch_index_list(index_list, data_file.root.data.shape[-3:], patch_shape, patch_overlap,\n",
    "                                             patch_start_offset)\n",
    "        count = 0\n",
    "        \n",
    "        for index in index_list:\n",
    "            x_list = list()\n",
    "            y_list = list()\n",
    "            #데이터 파일의 데이터를 feature과 target data 리스트들에 추가하는 함수\n",
    "            add_data(x_list, y_list, data_file, index, skip_blank=skip_blank, patch_shape=patch_shape)\n",
    "            #데이터파일로부터, 추가될 데이터가 있으면\n",
    "            if len(x_list) > 0:\n",
    "                count += 1\n",
    "                #얘가 패치 수가 된다.\n",
    "        return count\n",
    "    #patch_shape값이 안들어 오면\n",
    "    else:\n",
    "        #얘가 패치 수가 된다.\n",
    "        return len(index_list)\n",
    "\n",
    "\n",
    "#data_generator 함수와 get_number_of_patches함수에서 사용, patch의 인덱스 list를 생성하는 함수\n",
    "def create_patch_index_list(index_list, image_shape, patch_shape, patch_overlap, patch_start_offset=None):\n",
    "    patch_index = list()\n",
    "    for index in index_list:\n",
    "        #만약, patch_start_offset이 None이 아니면\n",
    "        if patch_start_offset is not None:\n",
    "            #랜덤으로 start offset값을 음수값으로 받아, random_start_offset 값에 저장하고,\n",
    "            random_start_offset = np.negative(get_random_nd_index(patch_start_offset))\n",
    "            #코너나, 경계값에서, patch를 어떻게 처리할지에 대해 정의된 compute_patch_indices 함수를 실행시키면,  get_set_of_patch_indices\n",
    "            #함수를 return 하여, 차원 뻥튀기를 해준다.\n",
    "            patches = compute_patch_indices(image_shape, patch_shape, overlap=patch_overlap, start=random_start_offset)\n",
    "        #그게 아니라면, start=None값으로 실행한다.\n",
    "        else:\n",
    "            patches = compute_patch_indices(image_shape, patch_shape, overlap=patch_overlap)\n",
    "        #patch_index에 다음을 추가\n",
    "        patch_index.extend(itertools.product([index], patches))\n",
    "        #patch_index를 반환\n",
    "    return patch_index\n",
    "\n",
    "#add_data(), def data_generator와 def get_number_of_patches에 사용되는 함수이다.\n",
    "#데이터 파일의 데이터를 feature과 target data 리스트들에 추가하는 함수\n",
    "# x_list : 데이터파일로부터 추가될 데이터가 있는 데이터 목록\n",
    "# y_list : 데이터파일로부터 추가될 target 데이터가 있는 데이터 목록\n",
    "# data_file : hdf5 데이터 파일\n",
    "# index : 데이터로부터 추출한 데이터파일들의 인덱스\n",
    "# augment : 만약, True면, 데이터들은 augmentation parameter(augment_flip과 augment_distortion_factor)에 따라 augment 된다. 즉, augment 사용여부\n",
    "# augment_flip : 만약, True면, x, y, z축을 따라 랜덤하게 뒤집힌다.\n",
    "# augment_distortion_factor : 만약, True면, 원본과의 표준편차를 구한다. None, False, 0으로 세팅하면, 이 방식으로 왜곡된 데이터가, 추가되는것을 막을 수 있다.\n",
    "# permute : 무작위로 데이터를 치환, 데이터는 반드시 3차원 큐브형태여야한다.\n",
    "def add_data(x_list, y_list, data_file, index, augment=False, augment_flip=False, augment_distortion_factor=0.25,\n",
    "             patch_shape=False, skip_blank=True, permute=False):\n",
    "    \"\"\"\n",
    "    Adds data from the data file to the given lists of feature and target data\n",
    "    :param skip_blank: Data will not be added if the truth vector is all zeros (default is True).\n",
    "    :param patch_shape: Shape of the patch to add to the data lists. If None, the whole image will be added.\n",
    "    :param x_list: list of data to which data from the data_file will be appended.\n",
    "    :param y_list: list of data to which the target data from the data_file will be appended.\n",
    "    :param data_file: hdf5 data file.\n",
    "    :param index: index of the data file from which to extract the data.\n",
    "    :param augment: if True, data will be augmented according to the other augmentation parameters (augment_flip and\n",
    "    augment_distortion_factor)\n",
    "    :param augment_flip: if True and augment is True, then the data will be randomly flipped along the x, y and z axis\n",
    "    :param augment_distortion_factor: if augment is True, this determines the standard deviation from the original\n",
    "    that the data will be distorted (in a stretching or shrinking fashion). Set to None, False, or 0 to prevent the\n",
    "    augmentation from distorting the data in this way.\n",
    "    :param permute: will randomly permute the data (data must be 3D cube)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data, truth = get_data_from_file(data_file, index, patch_shape=patch_shape)\n",
    "    if augment:\n",
    "        if patch_shape is not None:\n",
    "            affine = data_file.root.affine[index[0]]\n",
    "        else:\n",
    "            affine = data_file.root.affine[index]\n",
    "        data, truth = augment_data(data, truth, affine, flip=augment_flip, scale_deviation=augment_distortion_factor)\n",
    "\n",
    "    if permute:\n",
    "        if data.shape[-3] != data.shape[-2] or data.shape[-2] != data.shape[-1]:\n",
    "            raise ValueError(\"To utilize permutations, data array must be in 3D cube shape with all dimensions having \"\n",
    "                             \"the same length.\")\n",
    "        data, truth = random_permutation_x_y(data, truth[np.newaxis])\n",
    "    else:\n",
    "        truth = truth[np.newaxis]\n",
    "\n",
    "    if not skip_blank or np.any(truth != 0):\n",
    "        x_list.append(data)\n",
    "        y_list.append(truth)\n",
    "\n",
    "#위의 add_data에서 사용하기 위해, 정의된 함수, 파일로부터, 데이터를 얻어오는 함수\n",
    "#쉽게 말해, patch_shape값의 여부에 따라, data_file로부터 데이터를 얻어, data와 truth으로 나누고\n",
    "#그것을 다시, get_patch_from_3d_data에 넣어, patch값을 얻어, 반환하거나, patch_shape=None이면, data_file.root.data로부터 얻어서 반환\n",
    "def get_data_from_file(data_file, index, patch_shape=None):\n",
    "    #만약, patch_shape 값이 들어오면\n",
    "    if patch_shape:\n",
    "        #데이터로부터, 추출한 데이터 파일들의 인덱스, index, patch_index에 저장한다.\n",
    "        index, patch_index = index\n",
    "        #get_data_from_file을 실행해서, 얻은 값인, x, y값을 data와 truth에 넣어준다.\n",
    "        data, truth = get_data_from_file(data_file, index, patch_shape=None)\n",
    "        #x에 3d data에서 patch를 얻어온 값을 저장\n",
    "        x = get_patch_from_3d_data(data, patch_shape, patch_index)\n",
    "        #y에 3d truth에서 patch를 얻어온 값을 저장\n",
    "        y = get_patch_from_3d_data(truth, patch_shape, patch_index)\n",
    "        \n",
    "        #patch_shape값이 들어오지 않으면\n",
    "    else:\n",
    "        x, y = data_file.root.data[index], data_file.root.truth[index, 0]\n",
    "    return x, y\n",
    "\n",
    "#data 변환\n",
    "def convert_data(x_list, y_list, n_labels=1, labels=None):\n",
    "    x = np.asarray(x_list)\n",
    "    y = np.asarray(y_list)\n",
    "    if n_labels == 1:\n",
    "        y[y > 0] = 1\n",
    "    elif n_labels > 1:\n",
    "        y = get_multi_class_labels(y, n_labels=n_labels, labels=labels)\n",
    "    return x, y\n",
    "\n",
    "#convert_data 에서 사용되는 함수\n",
    "#label map을 이진label들로 변환하기 위한 함수\n",
    "#data : shape를 가진 label map을 포함하는 numpy 배열\n",
    "#n_labels : label들의 수\n",
    "#labels : label들의 정수값\n",
    "#shape의 이진 numpy 배열\n",
    "def get_multi_class_labels(data, n_labels, labels=None):\n",
    "    \"\"\"\n",
    "    Translates a label map into a set of binary labels.\n",
    "    :param data: numpy array containing the label map with shape: (n_samples, 1, ...).\n",
    "    :param n_labels: number of labels.\n",
    "    :param labels: integer values of the labels.\n",
    "    :return: binary numpy array of shape: (n_samples, n_labels, ...)\n",
    "    \"\"\"\n",
    "    #data와 label들의 수로, new_shape를 만든다.\n",
    "    new_shape = [data.shape[0], n_labels] + list(data.shape[2:])\n",
    "    #newshape의 크기만큼, zero array를 생성하여, 초기화 한다.\n",
    "    y = np.zeros(new_shape, np.int8)\n",
    "    #다음 조건을 만족하면, 해당 인덱스에 1값으로 대체한다.\n",
    "    #라벨이 있는 부분, 즉, 병변이라고 판단되는 라벨에, 1값을 넣어주는 y array 생성\n",
    "    for label_index in range(n_labels):\n",
    "        if labels is not None:\n",
    "            y[:, label_index][data[:, 0] == labels[label_index]] = 1\n",
    "        else:\n",
    "            y[:, label_index][data[:, 0] == (label_index + 1)] = 1\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:55:38.329266Z",
     "start_time": "2020-05-28T18:55:38.297265Z"
    }
   },
   "outputs": [],
   "source": [
    "#metrics.py\n",
    "#하나 이상의 인수가 이미 채워진 함수의 새 버전을 만들기 위해 사용되는 모듈\n",
    "from functools import partial\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "#dice 계수를 구하는 함수\n",
    "def dice_coefficient(y_true, y_pred, smooth=1.):\n",
    "    y_true_f = tf.cast(K.flatten(y_true), tf.float32, name=None)# K.flatten(y_true)\n",
    "    y_pred_f = tf.cast(K.flatten(y_pred), tf.float32, name=None)\n",
    "    #J.Lee: original: y_pred_f = K.flatten(y_pred)\n",
    "    #to prevent Value error. original y_pred is int8 and cannot be multiplied with fp32(y_true)\n",
    "    \n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "#dice 계수 loss\n",
    "def dice_coefficient_loss(y_true, y_pred):\n",
    "    return -dice_coefficient(y_true, y_pred)\n",
    "\n",
    "#weighted_dice_coefficient를 구하는 함수\n",
    "def weighted_dice_coefficient(y_true, y_pred, axis=(-3, -2, -1), smooth=0.00001):\n",
    "    \"\"\"\n",
    "    Weighted dice coefficient. Default axis assumes a \"channels first\" data structure\n",
    "    :param smooth:\n",
    "    :param y_true:\n",
    "    :param y_pred:\n",
    "    :param axis:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return K.mean(2. * (K.sum(y_true * y_pred,\n",
    "                              axis=axis) + smooth/2)/(K.sum(y_true,\n",
    "                                                            axis=axis) + K.sum(y_pred,\n",
    "                                                                               axis=axis) + smooth))\n",
    "\n",
    "#weighted_dice_coefficient를 구하는 함수, 그냥 부호만 -\n",
    "def weighted_dice_coefficient_loss(y_true, y_pred):\n",
    "    return -weighted_dice_coefficient(y_true, y_pred)\n",
    "\n",
    "#label_wise_dice_coefficeint\n",
    "def label_wise_dice_coefficient(y_true, y_pred, label_index):\n",
    "    return dice_coefficient(y_true[:, label_index], y_pred[:, label_index])\n",
    "\n",
    "\n",
    "def get_label_dice_coefficient_function(label_index):\n",
    "    f = partial(label_wise_dice_coefficient, label_index=label_index)\n",
    "    f.__setattr__('__name__', 'label_{0}_dice_coef'.format(label_index))\n",
    "    return f\n",
    "\n",
    "\n",
    "dice_coef = dice_coefficient\n",
    "dice_coef_loss = dice_coefficient_loss\n",
    "\n",
    "\n",
    "#training.py\n",
    "\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# from unet3d.metrics import (dice_coefficient, dice_coefficient_loss, dice_coef, dice_coef_loss,\n",
    "#                             weighted_dice_coefficient_loss, weighted_dice_coefficient)\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "# J.Lee: K.set_image_dim_ordering('th') -> K.set_image_data_format('channels_first') for use in keras ver2.x\n",
    "\n",
    "\n",
    "# learning rate schedule\n",
    "def step_decay(epoch, initial_lrate, drop, epochs_drop):\n",
    "    return initial_lrate * math.pow(drop, math.floor((1+epoch)/float(epochs_drop)))\n",
    "\n",
    "\n",
    "def get_callbacks(model_file, initial_learning_rate=0.0001, learning_rate_drop=0.5, learning_rate_epochs=None,\n",
    "                  learning_rate_patience=50,\n",
    "                  logging_file= data_dir+\"training.log\", #J.Lee:original:logging_file=\"training.log\"\n",
    "                  verbosity=1, early_stopping_patience=None):\n",
    "    callbacks = list()\n",
    "    callbacks.append(ModelCheckpoint(model_file, monitor='loss', #J.Lee:original: monitor='val_loss'(default)\n",
    "                                     save_best_only=True))\n",
    "    callbacks.append(CSVLogger(logging_file, append=True))\n",
    "    if learning_rate_epochs:\n",
    "        callbacks.append(LearningRateScheduler(partial(step_decay, initial_lrate=initial_learning_rate,\n",
    "                                                       drop=learning_rate_drop, epochs_drop=learning_rate_epochs)))\n",
    "    else:\n",
    "        callbacks.append(ReduceLROnPlateau(factor=learning_rate_drop, patience=learning_rate_patience,\n",
    "                                           verbose=verbosity))\n",
    "    if early_stopping_patience:\n",
    "        callbacks.append(EarlyStopping(verbose=verbosity, patience=early_stopping_patience))\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "def load_old_model(model_file):\n",
    "    print(\"Loading pre-trained model\")\n",
    "    custom_objects = {'dice_coefficient_loss': dice_coefficient_loss, 'dice_coefficient': dice_coefficient,\n",
    "                      'dice_coef': dice_coef, 'dice_coef_loss': dice_coef_loss,\n",
    "                      'weighted_dice_coefficient': weighted_dice_coefficient,\n",
    "                      'weighted_dice_coefficient_loss': weighted_dice_coefficient_loss}\n",
    "    try:\n",
    "        from keras_contrib.layers import InstanceNormalization\n",
    "        custom_objects[\"InstanceNormalization\"] = InstanceNormalization\n",
    "    except ImportError:\n",
    "        pass\n",
    "    try:\n",
    "        return load_model(model_file, custom_objects=custom_objects)\n",
    "    except ValueError as error:\n",
    "        if 'InstanceNormalization' in str(error):\n",
    "            raise ValueError(str(error) + \"\\n\\nPlease install keras-contrib to use InstanceNormalization:\\n\"\n",
    "                                          \"'pip install git+https://www.github.com/keras-team/keras-contrib.git'\")\n",
    "        else:\n",
    "            raise error\n",
    "\n",
    "\n",
    "def train_model(model, model_file, training_generator, validation_generator, steps_per_epoch, validation_steps,\n",
    "                initial_learning_rate=0.001, learning_rate_drop=0.5, learning_rate_epochs=None, n_epochs=500,\n",
    "                learning_rate_patience=20, early_stopping_patience=None):\n",
    "    \"\"\"\n",
    "    Train a Keras model.\n",
    "    :param early_stopping_patience: If set, training will end early if the validation loss does not improve after the\n",
    "    specified number of epochs.\n",
    "    :param learning_rate_patience: If learning_rate_epochs is not set, the learning rate will decrease if the validation\n",
    "    loss does not improve after the specified number of epochs. (default is 20)\n",
    "    :param model: Keras model that will be trained.\n",
    "    :param model_file: Where to save the Keras model.\n",
    "    :param training_generator: Generator that iterates through the training data.\n",
    "    :param validation_generator: Generator that iterates through the validation data.\n",
    "    :param steps_per_epoch: Number of batches that the training generator will provide during a given epoch.\n",
    "    :param validation_steps: Number of batches that the validation generator will provide during a given epoch.\n",
    "    :param initial_learning_rate: Learning rate at the beginning of training.\n",
    "    :param learning_rate_drop: How much at which to the learning rate will decay.\n",
    "    :param learning_rate_epochs: Number of epochs after which the learning rate will drop.\n",
    "    :param n_epochs: Total number of epochs to train the model.\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    model.fit(training_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=n_epochs,\n",
    "                    validation_data=None, #J.LEE :original: validation_generator,\n",
    "                    validation_steps=validation_steps,\n",
    "                    callbacks=get_callbacks(model_file,\n",
    "                                            initial_learning_rate=initial_learning_rate,\n",
    "                                            learning_rate_drop=learning_rate_drop,\n",
    "                                            learning_rate_epochs=learning_rate_epochs,\n",
    "                                            learning_rate_patience=learning_rate_patience,\n",
    "                                            early_stopping_patience=early_stopping_patience))\n",
    "# J.Lee:original:\n",
    "#     model.fit_generator(generator=training_generator,\n",
    "#                         steps_per_epoch=steps_per_epoch,\n",
    "#                         epochs=n_epochs,\n",
    "#                         validation_data=validation_generator,\n",
    "#                         validation_steps=validation_steps,\n",
    "#                         callbacks=get_callbacks(model_file,\n",
    "#                                                 initial_learning_rate=initial_learning_rate,\n",
    "#                                                 learning_rate_drop=learning_rate_drop,\n",
    "#                                                 learning_rate_epochs=learning_rate_epochs,\n",
    "#                                                 learning_rate_patience=learning_rate_patience,\n",
    "#                                                 early_stopping_patience=early_stopping_patience))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:55:38.507267Z",
     "start_time": "2020-05-28T18:55:38.330264Z"
    }
   },
   "outputs": [],
   "source": [
    "#model/unet.py\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Input, Model #J.Lee:original_code: from keras.engine import Input, Model\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, UpSampling3D, Activation, BatchNormalization, PReLU, Conv3DTranspose\n",
    "#J.Lee:original_code: Deconvolution3D instead of Conv3DTranspose\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#import tensorflow_addons as tfa #J.Lee: extracode added for InstanceNormalization \n",
    "\n",
    "# from unet3d.metrics import dice_coefficient_loss, get_label_dice_coefficient_function, dice_coefficient\n",
    "\n",
    "K.set_image_data_format(\"channels_first\")\n",
    "\n",
    "try:\n",
    "    from keras.engine import merge\n",
    "except ImportError:\n",
    "    from tensorflow.keras.layers import concatenate \n",
    "    #J.Lee:original_code: from keras.layers.merge import concatenate \n",
    "\n",
    "\n",
    "# def unet_model_3d(input_shape, pool_size=(2, 2, 2), n_labels=1, initial_learning_rate=0.00001, deconvolution=False,\n",
    "#                   depth=4, n_base_filters=32, include_label_wise_dice_coefficients=False, metrics=dice_coefficient,\n",
    "#                   batch_normalization=False, activation_name=\"sigmoid\"):\n",
    "#     \"\"\"\n",
    "#     Builds the 3D UNet Keras model.f\n",
    "#     :param metrics: List metrics to be calculated during model training (default is dice coefficient).\n",
    "#     :param include_label_wise_dice_coefficients: If True and n_labels is greater than 1, model will report the dice\n",
    "#     coefficient for each label as metric.\n",
    "#     :param n_base_filters: The number of filters that the first layer in the convolution network will have. Following\n",
    "#     layers will contain a multiple of this number. Lowering this number will likely reduce the amount of memory required\n",
    "#     to train the model.\n",
    "#     :param depth: indicates the depth of the U-shape for the model. The greater the depth, the more max pooling\n",
    "#     layers will be added to the model. Lowering the depth may reduce the amount of memory required for training.\n",
    "#     :param input_shape: Shape of the input data (n_chanels, x_size, y_size, z_size). The x, y, and z sizes must be\n",
    "#     divisible by the pool size to the power of the depth of the UNet, that is pool_size^depth.\n",
    "#     :param pool_size: Pool size for the max pooling operations.\n",
    "#     :param n_labels: Number of binary labels that the model is learning.\n",
    "#     :param initial_learning_rate: Initial learning rate for the model. This will be decayed during training.\n",
    "#     :param deconvolution: If set to True, will use transpose convolution(deconvolution) instead of up-sampling. This\n",
    "#     increases the amount memory required during training.\n",
    "#     :return: Untrained 3D UNet Model\n",
    "#     \"\"\"\n",
    "#     inputs = Input(input_shape)\n",
    "#     current_layer = inputs\n",
    "#     levels = list()\n",
    "\n",
    "#     # add levels with max pooling\n",
    "#     for layer_depth in range(depth):\n",
    "#         layer1 = create_convolution_block(input_layer=current_layer, n_filters=n_base_filters*(2**layer_depth),\n",
    "#                                           batch_normalization=batch_normalization)\n",
    "#         layer2 = create_convolution_block(input_layer=layer1, n_filters=n_base_filters*(2**layer_depth)*2,\n",
    "#                                           batch_normalization=batch_normalization)\n",
    "#         if layer_depth < depth - 1:\n",
    "#             current_layer = MaxPooling3D(pool_size=pool_size)(layer2)\n",
    "#             levels.append([layer1, layer2, current_layer])\n",
    "#         else:\n",
    "#             current_layer = layer2\n",
    "#             levels.append([layer1, layer2])\n",
    "\n",
    "#     # add levels with up-convolution or up-sampling\n",
    "#     for layer_depth in range(depth-2, -1, -1):\n",
    "#         up_convolution = get_up_convolution(pool_size=pool_size, deconvolution=deconvolution,\n",
    "#                                             #J.Lee:original: n_filters=current_layer._keras_shape[1]\n",
    "#                                             n_filters= tensorflow.keras.backend.int_shape(current_layer)[1])(current_layer)\n",
    "#         concat = concatenate([up_convolution, levels[layer_depth][1]], axis=1)\n",
    "#         current_layer = create_convolution_block(n_filters= tensorflow.keras.backend.int_shape(levels[layer_depth][1])[1],\n",
    "#                                                  # J.Lee:original: levels[layer_depth][1]._keras_shape[1],\n",
    "#                                                  input_layer=concat, batch_normalization=batch_normalization)\n",
    "#         current_layer = create_convolution_block(n_filters= tensorflow.keras.backend.int_shape(levels[layer_depth][1])[1],\n",
    "#                                                  # J.Lee:original: levels[layer_depth][1]._keras_shape[1],\n",
    "#                                                  input_layer=current_layer,\n",
    "#                                                  batch_normalization=batch_normalization)\n",
    "\n",
    "#     final_convolution = Conv3D(n_labels, (1, 1, 1))(current_layer)\n",
    "#     act = Activation(activation_name)(final_convolution)\n",
    "#     model = Model(inputs=inputs, outputs=act)\n",
    "\n",
    "#     if not isinstance(metrics, list):\n",
    "#         metrics = [metrics]\n",
    "\n",
    "#     if include_label_wise_dice_coefficients and n_labels > 1:\n",
    "#         label_wise_dice_metrics = [get_label_dice_coefficient_function(index) for index in range(n_labels)]\n",
    "#         if metrics:\n",
    "#             metrics = metrics + label_wise_dice_metrics\n",
    "#         else:\n",
    "#             metrics = label_wise_dice_metrics\n",
    "\n",
    "#     model.compile(optimizer=Adam(learning_rate=initial_learning_rate), loss=dice_coefficient_loss, metrics=metrics)\n",
    "#     return model\n",
    "\n",
    "\n",
    "def create_convolution_block(input_layer, n_filters, batch_normalization=False, kernel=(3, 3, 3), activation=None,\n",
    "                             padding='same', strides=(1, 1, 1), instance_normalization=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param strides:\n",
    "    :param input_layer:\n",
    "    :param n_filters:\n",
    "    :param batch_normalization:\n",
    "    :param kernel:\n",
    "    :param activation: Keras activation layer to use. (default is 'relu')\n",
    "    :param padding:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    layer = Conv3D(n_filters, kernel, padding=padding, strides=strides)(input_layer)\n",
    "    if batch_normalization:\n",
    "        layer = BatchNormalization(axis=1)(layer)\n",
    "    elif instance_normalization:\n",
    "        try:\n",
    "            #J.Lee:original: from keras_contrib.layers.normalization import InstanceNormalization\n",
    "            #from keras_contrib.layers.normalization import InstanceNormalization\n",
    "            from tensorflow_addons.layers import InstanceNormalization\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Install keras_contrib in order to use instance normalization.\"\n",
    "                              \"\\nTry: pip install git+https://www.github.com/farizrahman4u/keras-contrib.git\")\n",
    "        layer = InstanceNormalization(axis=1)(layer)\n",
    "    if activation is None:\n",
    "        return Activation('relu')(layer)\n",
    "    else:\n",
    "        return activation()(layer)\n",
    "\n",
    "\n",
    "def compute_level_output_shape(n_filters, depth, pool_size, image_shape):\n",
    "    \"\"\"\n",
    "    Each level has a particular output shape based on the number of filters used in that level and the depth or number \n",
    "    of max pooling operations that have been done on the data at that point.\n",
    "    :param image_shape: shape of the 3d image.\n",
    "    :param pool_size: the pool_size parameter used in the max pooling operation.\n",
    "    :param n_filters: Number of filters used by the last node in a given level.\n",
    "    :param depth: The number of levels down in the U-shaped model a given node is.\n",
    "    :return: 5D vector of the shape of the output node \n",
    "    \"\"\"\n",
    "    output_image_shape = np.asarray(np.divide(image_shape, np.power(pool_size, depth)), dtype=np.int32).tolist()\n",
    "    return tuple([None, n_filters] + output_image_shape)\n",
    "\n",
    "\n",
    "def get_up_convolution(n_filters, pool_size, kernel_size=(2, 2, 2), strides=(2, 2, 2),\n",
    "                       deconvolution=False):\n",
    "    if deconvolution:\n",
    "        return Conv3DTranspose(filters=n_filters, kernel_size=kernel_size,\n",
    "                               strides=strides) # J.Lee:original:Deconvolution3D\n",
    "    else:\n",
    "        return UpSampling3D(size=pool_size)\n",
    "\n",
    "    \n",
    "## isensee2017.py\n",
    "from functools import partial\n",
    "\n",
    "from tensorflow.keras.layers import Input, LeakyReLU, Add, UpSampling3D, Activation, SpatialDropout3D, Conv3D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# from .unet import create_convolution_block, concatenate\n",
    "# from ..metrics import weighted_dice_coefficient_loss\n",
    "\n",
    "\n",
    "create_convolution_block = partial(create_convolution_block, activation=LeakyReLU, instance_normalization=True)\n",
    "\n",
    "\n",
    "def isensee2017_model(input_shape=(4, 128, 128, 128), n_base_filters=16, depth=5, dropout_rate=0.3,\n",
    "                      n_segmentation_levels=3, n_labels=4, optimizer=Adam, initial_learning_rate=5e-4,\n",
    "                      loss_function=weighted_dice_coefficient_loss, activation_name=\"sigmoid\"):\n",
    "    \"\"\"\n",
    "    This function builds a model proposed by Isensee et al. for the BRATS 2017 competition:\n",
    "    https://www.cbica.upenn.edu/sbia/Spyridon.Bakas/MICCAI_BraTS/MICCAI_BraTS_2017_proceedings_shortPapers.pdf\n",
    "    This network is highly similar to the model proposed by Kayalibay et al. \"CNN-based Segmentation of Medical\n",
    "    Imaging Data\", 2017: https://arxiv.org/pdf/1701.03056.pdf\n",
    "    :param input_shape:\n",
    "    :param n_base_filters:\n",
    "    :param depth:\n",
    "    :param dropout_rate:\n",
    "    :param n_segmentation_levels:\n",
    "    :param n_labels:\n",
    "    :param optimizer:\n",
    "    :param initial_learning_rate:\n",
    "    :param loss_function:\n",
    "    :param activation_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    current_layer = inputs\n",
    "    level_output_layers = list()\n",
    "    level_filters = list()\n",
    "    for level_number in range(depth):\n",
    "        n_level_filters = (2**level_number) * n_base_filters\n",
    "        level_filters.append(n_level_filters)\n",
    "\n",
    "        if current_layer is inputs:\n",
    "            in_conv = create_convolution_block(current_layer, n_level_filters)\n",
    "        else:\n",
    "            in_conv = create_convolution_block(current_layer, n_level_filters, strides=(2, 2, 2))\n",
    "\n",
    "        context_output_layer = create_context_module(in_conv, n_level_filters, dropout_rate=dropout_rate)\n",
    "\n",
    "        summation_layer = Add()([in_conv, context_output_layer])\n",
    "        level_output_layers.append(summation_layer)\n",
    "        current_layer = summation_layer\n",
    "\n",
    "    segmentation_layers = list()\n",
    "    for level_number in range(depth - 2, -1, -1):\n",
    "        up_sampling = create_up_sampling_module(current_layer, level_filters[level_number])\n",
    "        concatenation_layer = concatenate([level_output_layers[level_number], up_sampling], axis=1)\n",
    "        localization_output = create_localization_module(concatenation_layer, level_filters[level_number])\n",
    "        current_layer = localization_output\n",
    "        if level_number < n_segmentation_levels:\n",
    "            segmentation_layers.insert(0, Conv3D(n_labels, (1, 1, 1))(current_layer))\n",
    "\n",
    "    output_layer = None\n",
    "    for level_number in reversed(range(n_segmentation_levels)):\n",
    "        segmentation_layer = segmentation_layers[level_number]\n",
    "        if output_layer is None:\n",
    "            output_layer = segmentation_layer\n",
    "        else:\n",
    "            output_layer = Add()([output_layer, segmentation_layer])\n",
    "\n",
    "        if level_number > 0:\n",
    "            output_layer = UpSampling3D(size=(2, 2, 2))(output_layer)\n",
    "\n",
    "    activation_block = Activation(activation_name)(output_layer)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=activation_block)\n",
    "    model.compile(optimizer=optimizer(lr=initial_learning_rate), loss=loss_function)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_localization_module(input_layer, n_filters):\n",
    "    convolution1 = create_convolution_block(input_layer, n_filters)\n",
    "    convolution2 = create_convolution_block(convolution1, n_filters, kernel=(1, 1, 1))\n",
    "    return convolution2\n",
    "\n",
    "\n",
    "def create_up_sampling_module(input_layer, n_filters, size=(2, 2, 2)):\n",
    "    up_sample = UpSampling3D(size=size)(input_layer)\n",
    "    convolution = create_convolution_block(up_sample, n_filters)\n",
    "    return convolution\n",
    "\n",
    "\n",
    "def create_context_module(input_layer, n_level_filters, dropout_rate=0.3, data_format=\"channels_first\"):\n",
    "    convolution1 = create_convolution_block(input_layer=input_layer, n_filters=n_level_filters)\n",
    "    dropout = SpatialDropout3D(rate=dropout_rate, data_format=data_format)(convolution1)\n",
    "    convolution2 = create_convolution_block(input_layer=dropout, n_filters=n_level_filters)\n",
    "    return convolution2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:55:38.512264Z",
     "start_time": "2020-05-28T18:55:38.508265Z"
    }
   },
   "outputs": [],
   "source": [
    "K.set_image_data_format(\"channels_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:55:38.585267Z",
     "start_time": "2020-05-28T18:55:38.513265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_17_1/BraTS19_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_17_1/BraTS19_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_17_1/BraTS19_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_17_1/BraTS19_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_17_1/BraTS19_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         0\n",
       "0  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_17_1/BraTS19_...\n",
       "1  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_17_1/BraTS19_...\n",
       "2  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_17_1/BraTS19_...\n",
       "3  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_17_1/BraTS19_...\n",
       "4  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_17_1/BraTS19_..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BraTS19_2013_17_1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         0\n",
       "0  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...\n",
       "1  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...\n",
       "2  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...\n",
       "3  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1...\n",
       "4  C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS1..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BraTS19_TMC_09043_1\n",
      "335 335\n"
     ]
    }
   ],
   "source": [
    "#This code substitute 'fetch_training_data_files'\n",
    "#aware that modality_order[:4] is equivalant to config[\"all_modalities\"]\n",
    "\n",
    "#modified from N.Freidman's preprocessing code\n",
    "data_dir = 'C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/'\n",
    "DATA_HGG = data_dir+'HGG/'\n",
    "DATA_LGG = data_dir+'LGG/'\n",
    "\n",
    "def modality_sorter(file_path, modality_order = ['flair', 't1','t1ce', 't2', 'seg']):\n",
    "    '''used as key to sort {file path} by {modality_order}'''\n",
    "    for i, modality in enumerate(modality_order):\n",
    "        if modality in file_path: return i\n",
    "\n",
    "hgg_paths = []\n",
    "hgg_subject_ids = []\n",
    "for dirpath, dirnames, files in os.walk(DATA_HGG):\n",
    "    if len(dirnames)>0 :hgg_subject_ids=dirnames\n",
    "    if ('BraTS19' in dirpath):\n",
    "        file_paths = [dirpath+'/'+file for file in files if ('nii.gz' in file) and ('nb4' not in file)] # J.Lee: include N4Bias images if you want\n",
    "        file_paths.sort(key=modality_sorter)\n",
    "        hgg_paths.append(tuple(file_paths))\n",
    "\n",
    "lgg_paths = []\n",
    "lgg_subject_ids = []\n",
    "for dirpath, dirnames, files in os.walk(DATA_LGG):\n",
    "    if len(dirnames)>0 :lgg_subject_ids=dirnames\n",
    "    if ('BraTS19' in dirpath):\n",
    "        file_paths = [dirpath+'/'+file for file in files if ('nii.gz' in file) and ('nb4' not in file)] # J.Lee: include N4Bias images if you want\n",
    "        file_paths.sort(key=modality_sorter)\n",
    "        lgg_paths.append(tuple(file_paths))\n",
    "\n",
    "training_files = hgg_paths+lgg_paths\n",
    "subject_ids = hgg_subject_ids+lgg_subject_ids\n",
    "        \n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "pd.set_option('max_colwidth', 120)\n",
    "\n",
    "display(pd.DataFrame(training_files[5]))\n",
    "print(subject_ids[5])\n",
    "display(pd.DataFrame(training_files[-1]))\n",
    "print(subject_ids[-1])\n",
    "print(len(training_files), len(subject_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-28T18:55:33.610Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# from unet3d.data import write_data_to_file, open_data_file\n",
    "# from unet3d.generator import get_training_and_validation_generators\n",
    "# from unet3d.model import isensee2017_model\n",
    "# from unet3d.training import load_old_model, train_model\n",
    "\n",
    "\n",
    "config = dict()\n",
    "config[\"image_shape\"] = (128, 128, 128)  # This determines what shape the images will be cropped/resampled to.\n",
    "config[\"patch_shape\"] = None  # switch to None to train on the whole image\n",
    "config[\"labels\"] = (1, 2, 4)  # the label numbers on the input image\n",
    "config[\"n_base_filters\"] = 16\n",
    "config[\"n_labels\"] = len(config[\"labels\"])\n",
    "config[\"all_modalities\"] = ['flair', 't1','t1ce', 't2'] #J.Lee:original:[\"t1\", \"t1ce\", \"flair\", \"t2\"]\n",
    "config[\"training_modalities\"] = config[\"all_modalities\"]  # change this if you want to only use some of the modalities\n",
    "config[\"nb_channels\"] = len(config[\"training_modalities\"])\n",
    "if \"patch_shape\" in config and config[\"patch_shape\"] is not None:\n",
    "    config[\"input_shape\"] = tuple([config[\"nb_channels\"]] + list(config[\"patch_shape\"]))\n",
    "else:\n",
    "    config[\"input_shape\"] = tuple([config[\"nb_channels\"]] + list(config[\"image_shape\"]))\n",
    "config[\"truth_channel\"] = config[\"nb_channels\"]\n",
    "config[\"deconvolution\"] = True  # if False, will use upsampling instead of deconvolution\n",
    "\n",
    "config[\"batch_size\"] = 1\n",
    "config[\"validation_batch_size\"] = 2\n",
    "config[\"n_epochs\"] = 10 #500  # cutoff the training after this many epochs\n",
    "config[\"patience\"] = 10  # learning rate will be reduced after this many epochs if the validation loss is not improving\n",
    "config[\"early_stop\"] = 50  # training will be stopped after this many epochs without the validation loss improving\n",
    "config[\"initial_learning_rate\"] = 5e-4\n",
    "config[\"learning_rate_drop\"] = 0.5  # factor by which the learning rate will be reduced\n",
    "config[\"validation_split\"] = 0.8  # portion of the data that will be used for training\n",
    "config[\"flip\"] = False  # augments the data by randomly flipping an axis during\n",
    "config[\"permute\"] = True  # data shape must be a cube. Augments the data by permuting in various directions\n",
    "config[\"distort\"] = None  # switch to None if you want no distortion\n",
    "config[\"augment\"] = config[\"flip\"] or config[\"distort\"]\n",
    "config[\"validation_patch_overlap\"] = 0  # if > 0, during training, validation patches will be overlapping\n",
    "config[\"training_patch_start_offset\"] = (16, 16, 16)  # randomly offset the first patch index by up to this offset\n",
    "config[\"skip_blank\"] = True  # if True, then patches without any target will be skipped\n",
    "\n",
    "data_dir = 'C:/Users/user/Desktop/MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/'\n",
    "config[\"data_file\"] = data_dir+ \"brats_data.h5\"#os.path.abspath(\"brats_data.h5\")\n",
    "config[\"model_file\"] = data_dir+ \"isensee_2017_model.h5\"#os.path.abspath(\"isensee_2017_model.h5\")\n",
    "config[\"training_file\"] = data_dir+ \"isensee_training_ids.pkl\" #os.path.abspath(\"isensee_training_ids.pkl\")\n",
    "config[\"validation_file\"] = data_dir+ \"isensee_validation_ids.pkl\"#os.path.abspath(\"isensee_validation_ids.pkl\")\n",
    "config[\"overwrite\"] = False  # If True, will previous files. If False, will use previously written files.\n",
    "\n",
    "\n",
    "\n",
    "# def fetch_training_data_files(return_subject_ids=False):\n",
    "#     training_data_files = list()\n",
    "#     subject_ids = list()\n",
    "#     for subject_dir in glob.glob(os.path.join(os.path.dirname(__file__), \"data\", \"preprocessed\", \"*\", \"*\")):\n",
    "#         subject_ids.append(os.path.basename(subject_dir))\n",
    "#         subject_files = list()\n",
    "#         for modality in config[\"training_modalities\"] + [\"truth\"]:\n",
    "#             subject_files.append(os.path.join(subject_dir, modality + \".nii.gz\"))\n",
    "#         training_data_files.append(tuple(subject_files))\n",
    "#     if return_subject_ids:\n",
    "#         return training_data_files, subject_ids\n",
    "#     else:\n",
    "#         return training_data_files\n",
    "\n",
    "#J.Lee:comments on fetch_training_data_files\n",
    "#Naomi 코드의 os.walk로 만든것과 동일한 기능 (단, preprocess.py 로 처리 해놓은 데이터라고 가정하는듯;)\n",
    "#config에 적힌 값을 가지고 리스트안의 튜플을 만들어옴\n",
    "#BraTS19_2013_2_1/BraTS19_2013_2_1_t1_nb4.nii.gz 일때\n",
    "#폴더명인 BraTS19_2013_2_1은 subject_ids 리스트에 담기고\n",
    "#전체 절대경로인 C:/MICCAI_BraTS_2019_Data_Training/HGG/CBraTS19_2013_2_1/BraTS19_2013_2_1_t1_nb4.nii.gz\n",
    "# training_data_files리스트의 원소인 튜플의 원소로 들어감.\n",
    "\n",
    "overwrite=config[\"overwrite\"]\n",
    "# convert input images into an hdf5 file\n",
    "\n",
    "if overwrite or not os.path.exists(config[\"data_file\"]):\n",
    "    #J.Lee: training_files, subject_ids = fetch_training_data_files(return_subject_ids=True)\n",
    "    write_data_to_file(training_files, config[\"data_file\"], image_shape=config[\"image_shape\"],\n",
    "                       subject_ids=subject_ids)\n",
    "data_file_opened = open_data_file(config[\"data_file\"])\n",
    "\n",
    "if not overwrite and os.path.exists(config[\"model_file\"]):\n",
    "    model = load_old_model(config[\"model_file\"])\n",
    "else:\n",
    "    # instantiate new model\n",
    "    model = isensee2017_model(input_shape=config[\"input_shape\"], n_labels=config[\"n_labels\"],\n",
    "                              initial_learning_rate=config[\"initial_learning_rate\"],\n",
    "                              n_base_filters=config[\"n_base_filters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-28T18:55:33.613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 4, 128, 128, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_60 (Conv3D)              (None, 16, 128, 128, 1744        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_54 (Inst (None, 16, 128, 128, 32          conv3d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_54 (LeakyReLU)      (None, 16, 128, 128, 0           instance_normalization_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_61 (Conv3D)              (None, 16, 128, 128, 6928        leaky_re_lu_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_55 (Inst (None, 16, 128, 128, 32          conv3d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_55 (LeakyReLU)      (None, 16, 128, 128, 0           instance_normalization_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout3d_10 (SpatialDr (None, 16, 128, 128, 0           leaky_re_lu_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_62 (Conv3D)              (None, 16, 128, 128, 6928        spatial_dropout3d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_56 (Inst (None, 16, 128, 128, 32          conv3d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_56 (LeakyReLU)      (None, 16, 128, 128, 0           instance_normalization_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 16, 128, 128, 0           leaky_re_lu_54[0][0]             \n",
      "                                                                 leaky_re_lu_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_63 (Conv3D)              (None, 32, 64, 64, 6 13856       add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_57 (Inst (None, 32, 64, 64, 6 64          conv3d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)      (None, 32, 64, 64, 6 0           instance_normalization_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_64 (Conv3D)              (None, 32, 64, 64, 6 27680       leaky_re_lu_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_58 (Inst (None, 32, 64, 64, 6 64          conv3d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_58 (LeakyReLU)      (None, 32, 64, 64, 6 0           instance_normalization_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout3d_11 (SpatialDr (None, 32, 64, 64, 6 0           leaky_re_lu_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_65 (Conv3D)              (None, 32, 64, 64, 6 27680       spatial_dropout3d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_59 (Inst (None, 32, 64, 64, 6 64          conv3d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)      (None, 32, 64, 64, 6 0           instance_normalization_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 32, 64, 64, 6 0           leaky_re_lu_57[0][0]             \n",
      "                                                                 leaky_re_lu_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_66 (Conv3D)              (None, 64, 32, 32, 3 55360       add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_60 (Inst (None, 64, 32, 32, 3 128         conv3d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)      (None, 64, 32, 32, 3 0           instance_normalization_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_67 (Conv3D)              (None, 64, 32, 32, 3 110656      leaky_re_lu_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_61 (Inst (None, 64, 32, 32, 3 128         conv3d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_61 (LeakyReLU)      (None, 64, 32, 32, 3 0           instance_normalization_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout3d_12 (SpatialDr (None, 64, 32, 32, 3 0           leaky_re_lu_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_68 (Conv3D)              (None, 64, 32, 32, 3 110656      spatial_dropout3d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_62 (Inst (None, 64, 32, 32, 3 128         conv3d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_62 (LeakyReLU)      (None, 64, 32, 32, 3 0           instance_normalization_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 64, 32, 32, 3 0           leaky_re_lu_60[0][0]             \n",
      "                                                                 leaky_re_lu_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_69 (Conv3D)              (None, 128, 16, 16,  221312      add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_63 (Inst (None, 128, 16, 16,  256         conv3d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)      (None, 128, 16, 16,  0           instance_normalization_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_70 (Conv3D)              (None, 128, 16, 16,  442496      leaky_re_lu_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_64 (Inst (None, 128, 16, 16,  256         conv3d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)      (None, 128, 16, 16,  0           instance_normalization_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout3d_13 (SpatialDr (None, 128, 16, 16,  0           leaky_re_lu_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_71 (Conv3D)              (None, 128, 16, 16,  442496      spatial_dropout3d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_65 (Inst (None, 128, 16, 16,  256         conv3d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)      (None, 128, 16, 16,  0           instance_normalization_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 128, 16, 16,  0           leaky_re_lu_63[0][0]             \n",
      "                                                                 leaky_re_lu_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_72 (Conv3D)              (None, 256, 8, 8, 8) 884992      add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_66 (Inst (None, 256, 8, 8, 8) 512         conv3d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)      (None, 256, 8, 8, 8) 0           instance_normalization_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_73 (Conv3D)              (None, 256, 8, 8, 8) 1769728     leaky_re_lu_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_67 (Inst (None, 256, 8, 8, 8) 512         conv3d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)      (None, 256, 8, 8, 8) 0           instance_normalization_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout3d_14 (SpatialDr (None, 256, 8, 8, 8) 0           leaky_re_lu_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_74 (Conv3D)              (None, 256, 8, 8, 8) 1769728     spatial_dropout3d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_68 (Inst (None, 256, 8, 8, 8) 512         conv3d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)      (None, 256, 8, 8, 8) 0           instance_normalization_68[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 256, 8, 8, 8) 0           leaky_re_lu_66[0][0]             \n",
      "                                                                 leaky_re_lu_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_12 (UpSampling3D) (None, 256, 16, 16,  0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_75 (Conv3D)              (None, 128, 16, 16,  884864      up_sampling3d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_69 (Inst (None, 128, 16, 16,  256         conv3d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)      (None, 128, 16, 16,  0           instance_normalization_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 256, 16, 16,  0           add_17[0][0]                     \n",
      "                                                                 leaky_re_lu_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_76 (Conv3D)              (None, 128, 16, 16,  884864      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_70 (Inst (None, 128, 16, 16,  256         conv3d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)      (None, 128, 16, 16,  0           instance_normalization_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_77 (Conv3D)              (None, 128, 16, 16,  16512       leaky_re_lu_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_71 (Inst (None, 128, 16, 16,  256         conv3d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)      (None, 128, 16, 16,  0           instance_normalization_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_13 (UpSampling3D) (None, 128, 32, 32,  0           leaky_re_lu_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_78 (Conv3D)              (None, 64, 32, 32, 3 221248      up_sampling3d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_72 (Inst (None, 64, 32, 32, 3 128         conv3d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)      (None, 64, 32, 32, 3 0           instance_normalization_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 128, 32, 32,  0           add_16[0][0]                     \n",
      "                                                                 leaky_re_lu_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_79 (Conv3D)              (None, 64, 32, 32, 3 221248      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_73 (Inst (None, 64, 32, 32, 3 128         conv3d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)      (None, 64, 32, 32, 3 0           instance_normalization_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_80 (Conv3D)              (None, 64, 32, 32, 3 4160        leaky_re_lu_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_74 (Inst (None, 64, 32, 32, 3 128         conv3d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_74 (LeakyReLU)      (None, 64, 32, 32, 3 0           instance_normalization_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_14 (UpSampling3D) (None, 64, 64, 64, 6 0           leaky_re_lu_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_82 (Conv3D)              (None, 32, 64, 64, 6 55328       up_sampling3d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_75 (Inst (None, 32, 64, 64, 6 64          conv3d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_75 (LeakyReLU)      (None, 32, 64, 64, 6 0           instance_normalization_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 64, 64, 64, 6 0           add_15[0][0]                     \n",
      "                                                                 leaky_re_lu_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_83 (Conv3D)              (None, 32, 64, 64, 6 55328       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_76 (Inst (None, 32, 64, 64, 6 64          conv3d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_76 (LeakyReLU)      (None, 32, 64, 64, 6 0           instance_normalization_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_84 (Conv3D)              (None, 32, 64, 64, 6 1056        leaky_re_lu_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_77 (Inst (None, 32, 64, 64, 6 64          conv3d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_77 (LeakyReLU)      (None, 32, 64, 64, 6 0           instance_normalization_77[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_15 (UpSampling3D) (None, 32, 128, 128, 0           leaky_re_lu_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_86 (Conv3D)              (None, 16, 128, 128, 13840       up_sampling3d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_78 (Inst (None, 16, 128, 128, 32          conv3d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_78 (LeakyReLU)      (None, 16, 128, 128, 0           instance_normalization_78[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 32, 128, 128, 0           add_14[0][0]                     \n",
      "                                                                 leaky_re_lu_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_87 (Conv3D)              (None, 16, 128, 128, 13840       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_79 (Inst (None, 16, 128, 128, 32          conv3d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_79 (LeakyReLU)      (None, 16, 128, 128, 0           instance_normalization_79[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_81 (Conv3D)              (None, 3, 32, 32, 32 195         leaky_re_lu_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_88 (Conv3D)              (None, 16, 128, 128, 272         leaky_re_lu_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_16 (UpSampling3D) (None, 3, 64, 64, 64 0           conv3d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_85 (Conv3D)              (None, 3, 64, 64, 64 99          leaky_re_lu_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_80 (Inst (None, 16, 128, 128, 32          conv3d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 3, 64, 64, 64 0           up_sampling3d_16[0][0]           \n",
      "                                                                 conv3d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_80 (LeakyReLU)      (None, 16, 128, 128, 0           instance_normalization_80[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_17 (UpSampling3D) (None, 3, 128, 128,  0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_89 (Conv3D)              (None, 3, 128, 128,  51          leaky_re_lu_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 3, 128, 128,  0           up_sampling3d_17[0][0]           \n",
      "                                                                 conv3d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 3, 128, 128,  0           add_20[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,269,561\n",
      "Trainable params: 8,269,561\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-28T18:55:33.617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   input_3               []                   0        True float32\n",
      "1   conv3d_60             (16, 128, 128, 128)  1744     True float32\n",
      "2   instance_normalizati  (16, 128, 128, 128)  32       True float32\n",
      "3   leaky_re_lu_54        (16, 128, 128, 128)  0        True float32\n",
      "4   conv3d_61             (16, 128, 128, 128)  6928     True float32\n",
      "5   instance_normalizati  (16, 128, 128, 128)  32       True float32\n",
      "6   leaky_re_lu_55        (16, 128, 128, 128)  0        True float32\n",
      "7   spatial_dropout3d_10  (16, 128, 128, 128)  0        True float32\n",
      "8   conv3d_62             (16, 128, 128, 128)  6928     True float32\n",
      "9   instance_normalizati  (16, 128, 128, 128)  32       True float32\n",
      "10  leaky_re_lu_56        (16, 128, 128, 128)  0        True float32\n",
      "11  add_14                (16, 128, 128, 128)  0        True float32\n",
      "12  conv3d_63             (32, 64, 64, 64)     13856    True float32\n",
      "13  instance_normalizati  (32, 64, 64, 64)     64       True float32\n",
      "14  leaky_re_lu_57        (32, 64, 64, 64)     0        True float32\n",
      "15  conv3d_64             (32, 64, 64, 64)     27680    True float32\n",
      "16  instance_normalizati  (32, 64, 64, 64)     64       True float32\n",
      "17  leaky_re_lu_58        (32, 64, 64, 64)     0        True float32\n",
      "18  spatial_dropout3d_11  (32, 64, 64, 64)     0        True float32\n",
      "19  conv3d_65             (32, 64, 64, 64)     27680    True float32\n",
      "20  instance_normalizati  (32, 64, 64, 64)     64       True float32\n",
      "21  leaky_re_lu_59        (32, 64, 64, 64)     0        True float32\n",
      "22  add_15                (32, 64, 64, 64)     0        True float32\n",
      "23  conv3d_66             (64, 32, 32, 32)     55360    True float32\n",
      "24  instance_normalizati  (64, 32, 32, 32)     128      True float32\n",
      "25  leaky_re_lu_60        (64, 32, 32, 32)     0        True float32\n",
      "26  conv3d_67             (64, 32, 32, 32)     110656   True float32\n",
      "27  instance_normalizati  (64, 32, 32, 32)     128      True float32\n",
      "28  leaky_re_lu_61        (64, 32, 32, 32)     0        True float32\n",
      "29  spatial_dropout3d_12  (64, 32, 32, 32)     0        True float32\n",
      "30  conv3d_68             (64, 32, 32, 32)     110656   True float32\n",
      "31  instance_normalizati  (64, 32, 32, 32)     128      True float32\n",
      "32  leaky_re_lu_62        (64, 32, 32, 32)     0        True float32\n",
      "33  add_16                (64, 32, 32, 32)     0        True float32\n",
      "34  conv3d_69             (128, 16, 16, 16)    221312   True float32\n",
      "35  instance_normalizati  (128, 16, 16, 16)    256      True float32\n",
      "36  leaky_re_lu_63        (128, 16, 16, 16)    0        True float32\n",
      "37  conv3d_70             (128, 16, 16, 16)    442496   True float32\n",
      "38  instance_normalizati  (128, 16, 16, 16)    256      True float32\n",
      "39  leaky_re_lu_64        (128, 16, 16, 16)    0        True float32\n",
      "40  spatial_dropout3d_13  (128, 16, 16, 16)    0        True float32\n",
      "41  conv3d_71             (128, 16, 16, 16)    442496   True float32\n",
      "42  instance_normalizati  (128, 16, 16, 16)    256      True float32\n",
      "43  leaky_re_lu_65        (128, 16, 16, 16)    0        True float32\n",
      "44  add_17                (128, 16, 16, 16)    0        True float32\n",
      "45  conv3d_72             (256, 8, 8, 8)       884992   True float32\n",
      "46  instance_normalizati  (256, 8, 8, 8)       512      True float32\n",
      "47  leaky_re_lu_66        (256, 8, 8, 8)       0        True float32\n",
      "48  conv3d_73             (256, 8, 8, 8)       1769728  True float32\n",
      "49  instance_normalizati  (256, 8, 8, 8)       512      True float32\n",
      "50  leaky_re_lu_67        (256, 8, 8, 8)       0        True float32\n",
      "51  spatial_dropout3d_14  (256, 8, 8, 8)       0        True float32\n",
      "52  conv3d_74             (256, 8, 8, 8)       1769728  True float32\n",
      "53  instance_normalizati  (256, 8, 8, 8)       512      True float32\n",
      "54  leaky_re_lu_68        (256, 8, 8, 8)       0        True float32\n",
      "55  add_18                (256, 8, 8, 8)       0        True float32\n",
      "56  up_sampling3d_12      (256, 16, 16, 16)    0        True float32\n",
      "57  conv3d_75             (128, 16, 16, 16)    884864   True float32\n",
      "58  instance_normalizati  (128, 16, 16, 16)    256      True float32\n",
      "59  leaky_re_lu_69        (128, 16, 16, 16)    0        True float32\n",
      "60  concatenate_8         (256, 16, 16, 16)    0        True float32\n",
      "61  conv3d_76             (128, 16, 16, 16)    884864   True float32\n",
      "62  instance_normalizati  (128, 16, 16, 16)    256      True float32\n",
      "63  leaky_re_lu_70        (128, 16, 16, 16)    0        True float32\n",
      "64  conv3d_77             (128, 16, 16, 16)    16512    True float32\n",
      "65  instance_normalizati  (128, 16, 16, 16)    256      True float32\n",
      "66  leaky_re_lu_71        (128, 16, 16, 16)    0        True float32\n",
      "67  up_sampling3d_13      (128, 32, 32, 32)    0        True float32\n",
      "68  conv3d_78             (64, 32, 32, 32)     221248   True float32\n",
      "69  instance_normalizati  (64, 32, 32, 32)     128      True float32\n",
      "70  leaky_re_lu_72        (64, 32, 32, 32)     0        True float32\n",
      "71  concatenate_9         (128, 32, 32, 32)    0        True float32\n",
      "72  conv3d_79             (64, 32, 32, 32)     221248   True float32\n",
      "73  instance_normalizati  (64, 32, 32, 32)     128      True float32\n",
      "74  leaky_re_lu_73        (64, 32, 32, 32)     0        True float32\n",
      "75  conv3d_80             (64, 32, 32, 32)     4160     True float32\n",
      "76  instance_normalizati  (64, 32, 32, 32)     128      True float32\n",
      "77  leaky_re_lu_74        (64, 32, 32, 32)     0        True float32\n",
      "78  up_sampling3d_14      (64, 64, 64, 64)     0        True float32\n",
      "79  conv3d_82             (32, 64, 64, 64)     55328    True float32\n",
      "80  instance_normalizati  (32, 64, 64, 64)     64       True float32\n",
      "81  leaky_re_lu_75        (32, 64, 64, 64)     0        True float32\n",
      "82  concatenate_10        (64, 64, 64, 64)     0        True float32\n",
      "83  conv3d_83             (32, 64, 64, 64)     55328    True float32\n",
      "84  instance_normalizati  (32, 64, 64, 64)     64       True float32\n",
      "85  leaky_re_lu_76        (32, 64, 64, 64)     0        True float32\n",
      "86  conv3d_84             (32, 64, 64, 64)     1056     True float32\n",
      "87  instance_normalizati  (32, 64, 64, 64)     64       True float32\n",
      "88  leaky_re_lu_77        (32, 64, 64, 64)     0        True float32\n",
      "89  up_sampling3d_15      (32, 128, 128, 128)  0        True float32\n",
      "90  conv3d_86             (16, 128, 128, 128)  13840    True float32\n",
      "91  instance_normalizati  (16, 128, 128, 128)  32       True float32\n",
      "92  leaky_re_lu_78        (16, 128, 128, 128)  0        True float32\n",
      "93  concatenate_11        (32, 128, 128, 128)  0        True float32\n",
      "94  conv3d_87             (16, 128, 128, 128)  13840    True float32\n",
      "95  instance_normalizati  (16, 128, 128, 128)  32       True float32\n",
      "96  leaky_re_lu_79        (16, 128, 128, 128)  0        True float32\n",
      "97  conv3d_81             (3, 32, 32, 32)      195      True float32\n",
      "98  conv3d_88             (16, 128, 128, 128)  272      True float32\n",
      "99  up_sampling3d_16      (3, 64, 64, 64)      0        True float32\n",
      "100 conv3d_85             (3, 64, 64, 64)      99       True float32\n",
      "101 instance_normalizati  (16, 128, 128, 128)  32       True float32\n",
      "102 add_19                (3, 64, 64, 64)      0        True float32\n",
      "103 leaky_re_lu_80        (16, 128, 128, 128)  0        True float32\n",
      "104 up_sampling3d_17      (3, 128, 128, 128)   0        True float32\n",
      "105 conv3d_89             (3, 128, 128, 128)   51       True float32\n",
      "106 add_20                (3, 128, 128, 128)   0        True float32\n",
      "107 activation_2          (3, 128, 128, 128)   0        True float32\n",
      "Compute dtype: float16\n",
      "Variable dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Checking layers and their index before training\n",
    "for layer,i in zip(model.layers,range(0,len(model.layers))):\n",
    "    print(f\"{i: <3}\",\n",
    "          f\"{layer.name[:20]: <20} \",\n",
    "          f\"{str(layer.output_shape[1:5]): <20}\",\n",
    "          f\"{layer.count_params(): <8}\", #f\"{np.array(layer.get_weights()).size: <8}\",\n",
    "          f\"{layer.trainable}\",\n",
    "          layer.dtype)\n",
    "    \n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-28T18:55:33.620Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation split...\n",
      "Number of training steps:  268\n",
      "Number of validation steps:  34\n"
     ]
    }
   ],
   "source": [
    "# get training and testing generators\n",
    "train_generator, validation_generator, n_train_steps, n_validation_steps = get_training_and_validation_generators(\n",
    "    data_file_opened,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    data_split=config[\"validation_split\"],\n",
    "    overwrite=overwrite,\n",
    "    validation_keys_file=config[\"validation_file\"],\n",
    "    training_keys_file=config[\"training_file\"],\n",
    "    n_labels=config[\"n_labels\"],\n",
    "    labels=config[\"labels\"],\n",
    "    patch_shape=config[\"patch_shape\"],\n",
    "    validation_batch_size=config[\"validation_batch_size\"],\n",
    "    validation_patch_overlap=config[\"validation_patch_overlap\"],\n",
    "    training_patch_start_offset=config[\"training_patch_start_offset\"],\n",
    "    permute=config[\"permute\"],\n",
    "    augment=config[\"augment\"],\n",
    "    skip_blank=config[\"skip_blank\"],\n",
    "    augment_flip=config[\"flip\"],\n",
    "    augment_distortion_factor=config[\"distort\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-28T18:55:33.623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Conv3DBackpropInputOpV2 only supports NDHWC on the CPU.\n\t [[node gradient_tape/model_2/conv3d_89/Conv3DBackpropInputV2 (defined at <ipython-input-46-f7f6653b7673>:146) ]] [Op:__inference_train_function_82851]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-a72d66e079b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mlearning_rate_patience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"patience\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mearly_stopping_patience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"early_stop\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             n_epochs=10)#J.Lee:original: n_epochs=config[\"n_epochs\"])\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m#data_file_opened.close()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-f7f6653b7673>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, model_file, training_generator, validation_generator, steps_per_epoch, validation_steps, initial_learning_rate, learning_rate_drop, learning_rate_epochs, n_epochs, learning_rate_patience, early_stopping_patience)\u001b[0m\n\u001b[0;32m    144\u001b[0m                                             \u001b[0mlearning_rate_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m                                             \u001b[0mlearning_rate_patience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate_patience\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                                             early_stopping_patience=early_stopping_patience))\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;31m# J.Lee:original:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;31m#     model.fit_generator(generator=training_generator,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  Conv3DBackpropInputOpV2 only supports NDHWC on the CPU.\n\t [[node gradient_tape/model_2/conv3d_89/Conv3DBackpropInputV2 (defined at <ipython-input-46-f7f6653b7673>:146) ]] [Op:__inference_train_function_82851]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "# run training\n",
    "\n",
    "# run training\n",
    "train_model(model=model,\n",
    "            model_file=config[\"model_file\"],\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=n_train_steps,\n",
    "            validation_steps=n_validation_steps,\n",
    "            initial_learning_rate=config[\"initial_learning_rate\"],\n",
    "            learning_rate_drop=config[\"learning_rate_drop\"],\n",
    "            learning_rate_patience=config[\"patience\"],\n",
    "            early_stopping_patience=config[\"early_stop\"],\n",
    "            n_epochs=10)#J.Lee:original: n_epochs=config[\"n_epochs\"])\n",
    "#data_file_opened.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T11:28:12.011499Z",
     "start_time": "2020-05-25T11:28:12.007505Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-28T18:55:33.628Z"
    }
   },
   "outputs": [],
   "source": [
    "# n_train_steps 조절해서 validation 넘어가는거 좀 더 쉽게 확인해보기.\n",
    "# train_steps = 50\n",
    "# validation_steps = 20 정도만 해도 되지 않을까;\n",
    "\n",
    "#임비멘토님께 generator pickle화 관련 꼼수 있는지 여쭤보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-28T18:55:33.631Z"
    }
   },
   "outputs": [],
   "source": [
    "#data_file_opened.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
